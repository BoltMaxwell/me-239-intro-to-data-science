{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "**Due: 11/8/2020 on gradescope**\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 9-11 (inclusive).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - You can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope.\n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "import scipy.stats as st\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Estimating the mechanical properties of a plastic material from molecular dynamics simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure that [this](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/stress_strain.txt) dataset is visible from this Jupyter notebook.\n",
    "You may achieve this by either:\n",
    "\n",
    "+ Downloading the data file, putting it in your Google drive, mounting the drive, and changing to the directory of the file (see Problem 0 in [Homework](https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/homework/homework_03.ipynb); or\n",
    "+ Downloading the file to the working directory of this notebook with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PurdueMechanicalEngineering/me-297-intro-to-data-science/master/homework/stress_strain.txt'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's up to you what you choose to do.\n",
    "If the file is in the right place, the following code should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.loadtxt('stress_strain.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was generated using a molecular dynamics simulation of a plastic material (thanks to [Professor Alejandro Strachan](https://engineering.purdue.edu/MSE/people/ptProfile?id=33239) for sharing the data!).\n",
    "Specifically, Strachan's group did the following:\n",
    "- They took a rectangular chunk of the material and marked the position of each one of its atoms;\n",
    "- They started applying a tensile force along one dimension.\n",
    "The atoms are coupled together through electromagnetic forces and they must all satisfy Newton's law of motion.\n",
    "- For each value of the applied tensile force they marked the stress (force be unit area) in the middle of the materail and the corresponding strain of the material (percent enlogation in the pulling direction).\n",
    "- Eventually the material entered the plastic regime and then it broke.\n",
    "Here is a visualization of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:, 0] # Strain \n",
    "y = data[:, 1] # Stress in MPa\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'ro', markersize=2, label = 'Stress-strain data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for each particular value of the strain, you don't necessarily get a unique stress.\n",
    "This is because in molecular dynamics the atoms are jiggling around due to thermal effects.\n",
    "So there is always this \"jiggling\" noise when you are trying to measure the stress and the strain.\n",
    "We would like to process this noise in order to extract what is known as the [stress-strain curve](https://en.wikipedia.org/wiki/Stressâ€“strain_curve) of the material.\n",
    "The stress-strain curve is a macroscopic property of the the material which is affeted by the fine structure, e.g., the chemical bonds, the crystaline structure, any defects, etc.\n",
    "It is a required input to mechanics of materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Fitting the stress-strain curve in the elastic regime\n",
    "The very first part of the stress-strain curve should be linear.\n",
    "It is called the *elastic regime*.\n",
    "In that region, say $\\epsilon < \\epsilon_l=0.04$, the relationship between stress and strain is:\n",
    "$$\n",
    "\\sigma(\\epsilon) = E\\epsilon.\n",
    "$$\n",
    "The constant $E$ is known as the *Young modulus* of the material.\n",
    "Assume that you measure $\\epsilon$ without any noise, but your measured $\\sigma$ is noisy.\n",
    "\n",
    "First, extract the relevant data for this problem, split it into training and validation datasets, and visualize the training and validation datasets using different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The point at which the stress-strain curve stops being linear\n",
    "epsilon_l = 0.04\n",
    "# Relevant data (this is nice way to get the linear part of the stresses and straints)\n",
    "x_rel = x[x < 0.04]\n",
    "y_rel = y[x < 0.04]\n",
    "# Visualize to make sure you have the right data\n",
    "plt.figure()\n",
    "plt.plot(x_rel, y_rel, 'ro', markersize=2, label = 'Stress-strain data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by splitting the data into training and validation sets. We will use the validation set to test how good our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_rel, y_rel, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.I\n",
    "\n",
    "In the same plot, show both the training and the validation data.\n",
    "Make sure you name the axes properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.II\n",
    "\n",
    "Perform Bayesian linear regression using [scikit.learn.BayesianRidge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html).\n",
    "Note that this kind of regression model estimates the measurement noise as well as the hyper-parameter for the weights for you. You don't have to pick it by hand. It does this through a process called the evidence approximation. The details of the evidence approximation are rather advanced and not covered in the class.\n",
    "However, BayesianRidge works quite well as an out of the box Bayesian regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need the design matrix.\n",
    "# We know that the constant should be zero because of the physics of the problem.\n",
    "# So, let's force it to be zero by NOT including the constant term in our model\n",
    "Phi_train = # Write code do define the design matrix. It should be N x 1\n",
    "# Performing Bayesian linear regression using scikit learn\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "# Below is how you initialize the train the BayesianRidge model\n",
    "# You need to give it the design matrix and the output data\n",
    "# normalize=True simply scales the input and the output (strains and stresses) before fitting\n",
    "# (by subtracting the mean and dividing by the standard deviation)\n",
    "# Note that if you don't specify the fit_intercept=False below, BayesianRidge will\n",
    "# automatically add a constant offset to your model (which here should be zero)\n",
    "model = BayesianRidge(fit_intercept=False).fit(Phi_train, y_train)\n",
    "# The notation followed by BayesianRidge is different than the notation \n",
    "# we used in the class for Bayesian regression\n",
    "# In particular, what we called sigma (the square root of measurement noise variance)\n",
    "# they call 1 / model.alpha_\n",
    "# And what we called alpha (the regularization parameter for the weights),\n",
    "# they call 1 / model.lambda_\n",
    "# Here is how you can get the square root of the measurement noise variance\n",
    "sigma = np.sqrt(1.0 / model.alpha_)\n",
    "print('sigma = {0:1.2f}'.format(sigma))\n",
    "# Here is how you can get regularization parameter:\n",
    "alpha = np.sqrt(1.0 / model.lambda_)\n",
    "print('alpha = {0:1.2f}'.format(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.III\n",
    "Calculate the mean square error of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how you can make predictions using the model:\n",
    "y_valid_p_mean = model.predict(x_valid[:, None])\n",
    "MSE = np.mean((y_valid_p_mean - y_valid) ** 2)\n",
    "print('MSE = {0:1.2f}'.format(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.IV\n",
    "Make the observations vs predictions plot for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.V\n",
    "Visualize your epistemic and the aleatory uncertainty about the stress-strain curve in the elastic regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some strain points\n",
    "xx = np.linspace(0.0, 0.05, 100)\n",
    "# Use the model to get the predictive mean and standard deviation:\n",
    "yy_mean, yy_measured_std = model.predict(xx[:, None], return_std=True)\n",
    "# Separate the epistemic uncertainty\n",
    "yy_std = np.sqrt(yy_measured_std ** 2 - sigma**2)\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A. VI\n",
    "Visualize the posterior of the Young modulus E conditioned on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The posterior mean of the weights is here\n",
    "m_norm = model.coef_\n",
    "print(m_norm)\n",
    "# The posterior covariance matrix for the weights is here \n",
    "S_norm = model.sigma_\n",
    "print(S_norm)\n",
    "# Create a Normal random variable to represent this\n",
    "E_post = st.norm(loc=m_norm[0], scale=np.sqrt(S_norm[0, 0]))\n",
    "# Plot the PDF of E_post\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.VII\n",
    "Take five samples of stress-strain curve in the elastic regime and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "for i in range(5):\n",
    "    E = # your code here - take the sample\n",
    "    yy_sample = E * xx\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "# plot the data again\n",
    "ax.plot(x_train, y_train, 'kx', label='Observed data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.VIII\n",
    "\n",
    "Find the 95% centered credible interval for the Young modulus $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.IX\n",
    "If you had to pick a single value for the Young modulus $E$, what would it be and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit subjective, but I would pick the median of the distribution (which here happens to be the same as the max of the posterior and the mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Estimate the ultimate strength\n",
    "\n",
    "The pick of the stress-strain curve is known as the ultimate strength.\n",
    "We will like to estimate it.\n",
    "\n",
    "### Subpart B.I - Extract training and validation data\n",
    "\n",
    "Extract training and validation data from the **entire** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to visualize your split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.II - Model the entire stress-strain relationship.\n",
    "To do this, we will set up a generalized linear model that can capture the entire stress-strain relationship.\n",
    "Remember, you can use any model you want as soon as:\n",
    "+ it is linear in the parameters to be estimated,\n",
    "+ it clearly has a well-defined elastic regime (see Part A).\n",
    "\n",
    "I am going to help you set up the right model.\n",
    "We are goint to use the [Heavide step function](https://en.wikipedia.org/wiki/Heaviside_step_function) to turn on or off models for various ranges of $\\epsilon$. The idea is quite simple: We will use a linear model for the elastic regime and we are going to turn to a non-linear model for the non-linear regime.\n",
    "Here is a model that has the right form in the elastic regime and an arbitrary form in the non-linear regime:\n",
    "$$\n",
    "f(\\epsilon;E,\\mathbf{w}_g) = E\\epsilon \\left[(1 - H(\\epsilon - \\epsilon_l)\\right] + g(\\epsilon;\\mathbf{w}_g)H(\\epsilon - \\epsilon_l),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "H(x) = \\begin{cases}\n",
    "0,\\;\\text{if}\\;x < 0\\\\\n",
    "1,\\;\\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "and $g$ is any function linear in the parameters $\\mathbf{w}_g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any model you like for the non-linear regime, but let's use a polynomial of degree $d$:\n",
    "$$\n",
    "g(\\epsilon) = \\sum_{i=0}^{d} w_i \\epsilon^i.\n",
    "$$\n",
    "\n",
    "The full model can be expressed as:\n",
    "$$\n",
    "\\begin{split}\n",
    "f(\\epsilon) &= \n",
    "\\begin{cases}\n",
    "h(\\epsilon) = E \\epsilon,\\ \\epsilon < \\epsilon_l, \\\\\n",
    "g(\\epsilon) = \\sum_{i=0}^{d} w_i \\epsilon^i, \\epsilon \\geq \\epsilon_l\n",
    "\\end{cases}\\\\\n",
    "&= E\\epsilon \\left(1 - H(\\epsilon - \\epsilon_l)\\right) + \\sum_{i=0}^{d} w_i \\epsilon^iH(\\epsilon - \\epsilon_l).\n",
    "\\end{split}\n",
    "$$\n",
    "We could proceed with this model, but there is a small problem: It is discontinuous at $\\epsilon = \\epsilon_l$.\n",
    "This is unphysical. We can do better than that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model nice, we force the $h$ and $g$ to match up to the first derivative, i.e., we demand that:\n",
    "$$\n",
    "\\begin{split}\n",
    "h(\\epsilon_l) &= g(\\epsilon_l)\\\\\n",
    "h'(\\epsilon_l) &= g'(\\epsilon_l).\n",
    "\\end{split}\n",
    "$$\n",
    "The reason we include the first derivative is so that we don't have a kink in the stress-strain. That would also be unphysical.\n",
    "The two equations above become:\n",
    "$$\n",
    "\\begin{split}\n",
    "E\\epsilon_l &= \\sum_{i=0}^dw_i\\epsilon_l^i\\\\\n",
    "E &= \\sum_{i=1}^diw_i\\epsilon_l^{i-1}.\n",
    "\\end{split}\n",
    "$$\n",
    "We can use these two equations to eliminate two weights.\n",
    "Let's eliminate $w_0$ and $w_1$.\n",
    "All you have to do is express them in terms of $E$ and $w_2,\\dots,w_d$.\n",
    "So, there remain $d$ parameters to estimate.\n",
    "Let's get back to the stress-strain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stress-strain model was:\n",
    "$$\n",
    "f(\\epsilon) = E\\epsilon \\left(1 - H(\\epsilon - \\epsilon_l)\\right) + \\sum_{i=0}^{d} w_i \\epsilon^iH(\\epsilon - \\epsilon_l).\n",
    "$$\n",
    "We can now use the expressions for $w_0$ and $w_1$ to rewrite this using only all the other parameters.\n",
    "I am going to spare you the details...\n",
    "The end result is:\n",
    "$$\n",
    "f(\\epsilon) = E\\epsilon + \\sum_{i=2}^dw_i\\left[(i-1)\\epsilon_{l}^{i} - i \\epsilon \\epsilon_{l}^{i-1} + \\epsilon^i\\right]H(\\epsilon - \\epsilon_l).\n",
    "$$\n",
    "Okay.\n",
    "This is still a generalized linear model. This is nice.\n",
    "Write code for the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this code to make your model:\n",
    "def compute_design_matrix(Epsilon, epsilon_l, d):\n",
    "    \"\"\"\n",
    "    Computes the design matrix for the stress-strain curve problem.\n",
    "    \n",
    "    Arguments:\n",
    "        Epsilon     -     A 1D array of dimension N.\n",
    "        epsilon_l   -     The strain signifying the end of the elastic regime.\n",
    "        d           -     The polynomial degree.\n",
    "    \n",
    "    Returns:\n",
    "        A design matrix N x d\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    assert isinstance(Epsilon, np.ndarray)\n",
    "    assert Epsilon.ndim == 1, 'Pass the array as epsilon.flatten(), if it is two dimensional'\n",
    "    n = Epsilon.shape[0]\n",
    "    # The design matrix:\n",
    "    Phi = np.ndarray((n, d))\n",
    "    # The step function\n",
    "    Step = np.ones(n)\n",
    "    Step[Epsilon < epsilon_l] = 0\n",
    "    # Build the design matrix\n",
    "    Phi[:, 0] = # Your code here\n",
    "    for i in range(2, d+1):\n",
    "        Phi[:, i-1] = # Your code here\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the basis functions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "eps = np.linspace(0, x.max(), 100)\n",
    "Phis = compute_design_matrix(eps, epsilon_l, d)\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(eps, Phis)\n",
    "ax.set_xlabel('$\\epsilon$ (strain in %))')\n",
    "ax.set_ylabel('$\\phi_i(\\epsilon)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.III \n",
    "\n",
    "Fit the model using Bayesian regression and demonstrate that it works well by doing all the things we did above (MSE, observations vs predictions plot, standarized errors, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_train = compute_design_matrix(x_train, epsilon_l, d)\n",
    "model = # Your code here using BayesianRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the observations vs predictions plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.IV\n",
    "Visualize epistemic and aleatory uncertainty in the stess-strain relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.V - Extract the ultimate strength\n",
    "\n",
    "Now, you are going to quantify your epistemic uncertainty about the ultimate strength.\n",
    "The ultimate strength is the maximum of the stress-strain relationship.\n",
    "Since you have epistemic uncertainty about the stress-strain relationship, you also have epistemic uncertainty about the ultimate strength.\n",
    "\n",
    "Do the following:\n",
    "- Visualize posterior of the ultimate strength.\n",
    "- Find a 95% credible interval for the ultimate strength.\n",
    "- Pick a value for the ultimate strength.\n",
    "\n",
    "**Hint:**\n",
    "To characterize your epistemic uncertainty about the ultimate strength, you would have to do the following:\n",
    "- Define a dense set of strain points between 0 and 0.25.\n",
    "- Repeatedly:\n",
    "    + sample from the posterior of the weights of your model\n",
    "    + for each sample evaluate the stresses at the dense set of strain points defined earlier\n",
    "    + for each sampled stress vector, find the maximum. This is a sample of the ultimate strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posterior mean of the weights is here (this is for the normalized features, however)\n",
    "m_norm = model.coef_\n",
    "# The posterior covariance matrix for the weights is here (also for the normalized features)\n",
    "S_norm = model.sigma_ + 1e-6 * np.eye(model.sigma_.shape[0])\n",
    "# The posterior of the weights\n",
    "w_post = st.multivariate_normal(mean=m_norm, cov=S_norm)\n",
    "# The number of samples to take\n",
    "num_post_samples = 1000\n",
    "# Storing the ultimate strength of each one of the posterior samples\n",
    "ultimate_strength_post_samples = np.ndarray((num_post_samples,))\n",
    "# Start taking samples\n",
    "for n in range(num_post_samples):\n",
    "    w_sample = # Your code here\n",
    "    yy_sample = # Your code here\n",
    "    ultimate_strength_post_samples[n] = np.max(yy_sample)\n",
    "# Do the histogram of ultimate_strength_post_samples\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Optimizing the performance of a compressor\n",
    "\n",
    "In this problem we are going to need [this](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/compressor_data.xlsx) dataset. The dataset was kindly provided to us by [Professor Davide Ziviani](https://scholar.google.com/citations?user=gPdAtg0AAAAJ&hl=en).\n",
    "As before, you can either put it on your Google drive or just download it with the code segment below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PurdueMechanicalEngineering/me-297-intro-to-data-science/master/homework/compressor_data.xlsx'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is an Excell file, so we are going to need pandas to read it.\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('compressor_data.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are part of a an experimental study of a variable speed reciprocating compressor.\n",
    "The experimentalists varied two temperatures $T_e$ and $T_c$ (both in C) and they measured various other quantities.\n",
    "Our goal is to learn the map between $T_e$ and $T_c$ and measured Capacity and Power (both in W).\n",
    "First, let's see how you can extract only the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to extract the T_e and T_c columns and put them in a single numpy array\n",
    "X = data[['T_e','T_c']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to extract the Capacity\n",
    "y = data['Capacity'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the following multivariate polynomial model to **both the Capacity and the Power**:\n",
    "$$\n",
    "y = w_1 + w_2T_e + w_3 T_c + w_4 T_eT_c + w_5 T_e^2 + w_6T_c^2 + w_7 T_e^2T_c + w_8T_eT_c^2 + w_9 T_e^3 + w_{10}T_c^3 + \\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is a Gaussian noise term with unknown variance.\n",
    "**Hints:**\n",
    "+ You may use [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to construct the design matrix of your polynomial features. Do not program the design matrix by hand.\n",
    "+ You should split your data into training and validation and use various validation metrics to make sure that your models make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - The Capacity\n",
    "\n",
    "### Subpart A.I - Fit the capacity\n",
    "\n",
    "Please don't just fit blindly. Split in training and test and use all the usual diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33)\n",
    "# Here is how to make Polynomials features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "# Design matrix for train data\n",
    "Phi_train = poly.fit_transform(X_train)\n",
    "# Fit with Bayesian Ridge - Use normalize=True to let BayesianRidge\n",
    "# scale the inputs and outputs to reasonable values (it will subtract their empirical mean\n",
    "# and divide by their empirical standard deviation)\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean square error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations vs predictions plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.II\n",
    "\n",
    "What is the noise standard deviation you estimated for the Capacity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.sqrt(1.0 / model.alpha_)\n",
    "print('sigma = {0:1.2f} W'.format(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.I - Fit the Power\n",
    "\n",
    "Please don't just fit blindly. Split in training and test and use all the usual diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to extract the Capacity\n",
    "y = data['Power'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean square error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations vs predictions plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.II\n",
    "\n",
    "What is the noise standard deviation you estimated for the Power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Explaining the challenger disaster\n",
    "On January 28, 1986, the [Space Shuttle Challenger](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster) disintegrated after 73 seconds from launch.\n",
    "The failure can be traced on the rubber O-rings which were used to seal the joints of the solid rocket boosters (required to force the hot, high-pressure gases generated by the burning solid propelant through the nozzles thus producing thrust).\n",
    "\n",
    "It turns out that the performance of the O-ring material was particularly sensitive on the external temperature during launch.\n",
    "This [dataset](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/challenger_data.csv) contains records of different experiments with O-rings recorded at various times between 1981 and 1986.\n",
    "Download the data the usual way (either put them on Google drive or run the code cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PurdueMechanicalEngineering/me-297-intro-to-data-science/master/homework/challenger_data.csv'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a csv file, you should load it with pandas because it contains some special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('challenger_data.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the date of the record. The second column is the external temperature of that day in degrees F.\n",
    "The third column labeled ``Damage Incident`` is has a binary coding (0=no damage, 1=damage).\n",
    "The very last row is the day of the Challenger accident.\n",
    "\n",
    "We are going to use the first 23 rows to solve a binary classification problem that will give us the probability of an accident conditioned on the observed external temperature in degrees F. Before we proceed to the analysis of the data, let's clean the data up.\n",
    "\n",
    "First, we drop all the bad records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also don't need the last record. Just remember that the temperature the day of the Challenger accident was 31 degrees F.\n",
    "Remove the last record from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "clean_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the features and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_data['Temperature'].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clean_data['Damage Incident'].values.astype(np.int)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Perform logistic regression\n",
    "\n",
    "Perform logistic regression between the temperature ($x$) and the damage label ($y$).\n",
    "Do not bother doing a validation because there are not a lot of data.\n",
    "Just use a very simple model so that you don't overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "This is one of the cases, where we don't have a lot of data.\n",
    "So we are going to use everything for training.\n",
    "To avoid overfitting, we will use the simplest possible model.\n",
    "The model is:\n",
    "$$\n",
    "p(y|x,w) = \\operatorname{sigm}(w_0 + w_1 x),\n",
    "$$\n",
    "where $w_0$ and $w_1$ are parameters to be determined by data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it. Let's take a look at the parameters that that were found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a negative correlation between temperature and damage. Damage becomes more probable as temperature decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Plot the probability of damage as a function of temperature\n",
    "Plot the probability of damage as a function of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Decide whether or not to launch\n",
    "\n",
    "The temperature the day of the Challenger accident was 31 degrees F.\n",
    "Start by calculating the probability of damage at 31 degrees F.\n",
    "Then, use formal decision-making (i.e., define a cost matrix and make decisions by minimizing the expected loss) to decide whether or not to launch on that day.\n",
    "Also, plot your optimal decision as a function of the external temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here. As many code and text blocks as you need.* "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
