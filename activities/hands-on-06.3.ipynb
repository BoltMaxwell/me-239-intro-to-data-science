{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Activity 6.3: The Multivariate Normal - Full Covariance Case\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce the multivariate normal with full covariance.\n",
    "\n",
    "## The multivariate mormal - Full covariance case\n",
    "\n",
    "Consider the $N$-dimensional multivariate normal:\n",
    "$$\n",
    "\\mathbf{X} \\sim N\\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right),\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}$ is a $N$-dimensional vector, $\\boldsymbol{\\Sigma}$ is a *positive-definite matrix*.\n",
    "Let's plot contours and take samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# The mean vector\n",
    "mu = [1.0, 2.0]\n",
    "# The covariance matrix\n",
    "Sigma = np.array([[1.0, 0.9],\n",
    "                   [0.9, 1.0]])\n",
    "# The multivariate normal random vector\n",
    "X = st.multivariate_normal(mean=mu, cov=Sigma)\n",
    "\n",
    "# CONTOURS\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# The contours\n",
    "# Points along x1 dimension\n",
    "x1 = np.linspace(-3, 5, 64)\n",
    "# Points along x2 dimension\n",
    "x2 = np.linspace(-3, 5, 64)\n",
    "# Create grid\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "# Flattened values of grid points\n",
    "X_flat = np.hstack([X1.flatten()[:, None], X2.flatten()[:, None]])\n",
    "# PDF values\n",
    "pdf_X = X.pdf(X_flat).reshape(X1.shape)\n",
    "# Plot contours\n",
    "c = ax.contour(X1, X2, pdf_X)\n",
    "plt.colorbar(c, label='$p(\\mathbf{x})$')\n",
    "\n",
    "# SAMPLES\n",
    "num_samples = 500\n",
    "x_samples = X.rvs(size=num_samples)\n",
    "ax.plot(x_samples[:, 0], x_samples[:, 1], '.', markersize=2, label='Samples')\n",
    "# Plot the mean\n",
    "ax.plot(X.mean[0], X.mean[1], 'ro', label='$\\mu$')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "+ Rerun the steps above after moving $\\boldsymbol{\\mu}$ to $(0, 1)$. Observe how the contours of the PDF move.\n",
    "+ Rerun the steps above for $\\Sigma_{12} = \\Sigma_{21} = 0.1$. Observe how the contours of the PDF change.\n",
    "+ Rerun the steps above for $\\Sigma_{12} = \\Sigma_{21} = -0.9$. Observe how the contours of the PDF change.\n",
    "+ Rerun the steps above for $\\Sigma_{11} = 0.4$. Why does the code fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The covariance matrix must be positive definite so that $p(\\mathbf{x})$ has a well-defined, unique maximum\n",
    "\n",
    "In the lecture we argued that the covariance matrix $\\boldsymbol{\\Sigma}$ must be positive definite so that the PDF of $\\mathbf{X}$ has a unique maximum. Let's try to understand this step by step.\n",
    "First, what does it mean for a matrix to be positive definite.\n",
    "The mathematical definition stats that for any vector $\\mathbf{v}\\not=\\mathbf{0}$, the quantity $\\mathbf{v}^T\\boldsymbol{\\Sigma}\\mathbf{v}$ is positive.\n",
    "Let's just test this quantity for two different matrices with random vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A covariane matrix that we know works\n",
    "Sigma_good = np.array([[1.0, 0.9],\n",
    "                       [0.9, 1.0]])\n",
    "# A covariance matrix that we know does not work\n",
    "Sigma_bad = np.array([[0.4, 0.9],\n",
    "                      [0.9, 1.0]])\n",
    "# Take random vectors and compute at quantity\n",
    "num_vectors = 5000\n",
    "Q_good = np.ndarray((num_vectors,))\n",
    "Q_bad = np.ndarray((num_vectors,))\n",
    "for i in range(num_vectors):\n",
    "    v = np.random.randn(2)\n",
    "    Q_good[i] = np.dot(v, np.dot(Sigma_good, v))\n",
    "    Q_bad[i] = np.dot(v, np.dot(Sigma_bad, v))\n",
    "\n",
    "# Let's do the histograms of these quantities to see whether or not they are positive\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.hist(Q_good, density=True, alpha=0.5, bins=100, label='Positive definite $\\Sigma$')\n",
    "ax.hist(Q_bad, density=True, alpha=0.5, bins=100, label='Not positive definite $\\Sigma$')\n",
    "ax.set_xlabel(r'$v^T\\Sigma v$')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the non-positive definite matrix is giving us quite a few negative value.\n",
    "Is there a way to check if a matrix is positive definite without doing this random test?\n",
    "Yes, you just check if all the eigenvalues of the matrix are positive. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eigenvalues of Sigma_good', np.linalg.eigh(Sigma_good)[0])\n",
    "print('Eigenvalues of Sigma_bad', np.linalg.eigh(Sigma_bad)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... And you see that the second one has a negative eigenvlue.\n",
    "\n",
    "Finally, let's visualize the contour of the probability densty and see with our own eyes that it does not have a unique minimum when the matrix $\\boldsymbol{\\Sigma}$ is not positive definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pdf_mv(x, mu, Sigma):\n",
    "    \"\"\"\n",
    "    Computes the PDF of the multivariate Gaussian in a way that does not require\n",
    "    Sigma to be positive definite, so that you can see what happens.\n",
    "    \"\"\"\n",
    "    N = Sigma.shape[0]\n",
    "    # Just keep in mind that this is one of the most unstable ways of computing the log pdf\n",
    "    return -0.5 * N * np.log(2.0 * np.pi) - 0.5 * np.linalg.det(Sigma) - 0.5 * np.dot((x - mu), np.dot(np.linalg.inv(Sigma), x - mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTOUR\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# The contours\n",
    "# Points along x1 dimension\n",
    "x1 = np.linspace(-3, 5, 64)\n",
    "# Points along x2 dimension\n",
    "x2 = np.linspace(-3, 5, 64)\n",
    "# Create grid\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "# Flattened values of grid points\n",
    "X_flat = np.hstack([X1.flatten()[:, None], X2.flatten()[:, None]])\n",
    "# PDF values\n",
    "pdf_X_bad = np.ndarray((64, 64))\n",
    "for i in range(64):\n",
    "    for j in range(64):\n",
    "        pdf_X_bad[i, j] = log_pdf_mv(np.array([X1[i, j], X2[i, j]]), mu, Sigma_bad)\n",
    "# Plot contours\n",
    "c = ax.contour(X1, X2, pdf_X_bad)\n",
    "plt.colorbar(c, label='$\\log p(\\mathbf{x})$')\n",
    "# Add the mean\n",
    "ax.plot(X.mean[0], X.mean[1], 'ro', label='$\\mu$')\n",
    "ax.set_title('Not positive definite $\\Sigma$')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\boldsymbol{\\mu}$ is not a maximum of $\\log p(\\mathbf{x})$ but a saddle point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Rerun the code above for $\\Sigma_{12} = \\Sigma_{21} = -0.9$ (both for the \"good\" and the \"bad\" covariance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the multivariate normal with diagonal covariance using the standard normal\n",
    "\n",
    "In the lecture we showed that if $\\mathbf{Z}$ is an $N$-dimensional standard normal:\n",
    "$$\n",
    "\\mathbf{Z} \\sim N(0,\\mathbf{I}),\n",
    "$$\n",
    "where $\\mathbf{I}$ is the $N\\times N$ unit matrix (all zeros except the diagonal which is all ones), then the random vector:\n",
    "$$\n",
    "\\mathbf{X} = \\boldsymbol{\\mu} + \\mathbf{A}\\mathbf{Z},\n",
    "$$\n",
    "is a multivariate normal:\n",
    "$$\n",
    "\\mathbf{X} \\sim N\\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\right),\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}^T.\n",
    "$$\n",
    "Such a matrix $\\mathbf{A}$ is non-unique and is is called a \"square root\" of $\\boldsymbol{\\Sigma}$.\n",
    "The most commonnly used square root of $\\boldsymbol{\\Sigma}$, however, is the [Cholesky](https://www.youtube.com/watch?v=TprfUB3nI8Y) (and pronounced KOLESKI not TSOLESKI or SHOLESKI  -- at least by the people who taught me linear algebra!)\n",
    "In the Cholesky decomposition $\\mathbf{A}$ is a lower triangular matrix (everything above the diagonal is zero) and the diagonal contains only positive numbers.\n",
    "Let's find the Cholesky decomposition of a positive definite $\\boldsymbol{\\Sigma}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A covariane matrix that we know works\n",
    "Sigma = np.array([[1.0, 0.9],\n",
    "                 [0.9, 1.0]])\n",
    "A = np.linalg.cholesky(Sigma)\n",
    "print('A = ', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a sanity check let's see if A * A.T gives us Sigma\n",
    "print('A * A.T = ', np.dot(A, A.T))\n",
    "print('Compare to Sigma = ', Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now verify that if we sample $\\mathbf{Z}$ from $N(\\mathbf{0},\\mathbf{I})$ and evaluate $\\mathbf{X} = \\boldsymbol{\\mu} + \\mathbf{A}\\mathbf{Z}$, then $\\mathbf{X}$ will be distributed according to $N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multivariate normal that you want to study:\n",
    "# The mean vector\n",
    "mu = [1.0, 2.0]\n",
    "\n",
    "# Repeating Sigma here. Change it here for the questions:\n",
    "Sigma = np.array([[1.0, 0.9],\n",
    "                 [0.9, 1.0]])\n",
    "A = np.linalg.cholesky(Sigma)\n",
    "\n",
    "# Create the random variable using scipy.stats\n",
    "X = st.multivariate_normal(mean=mu, cov=Sigma)\n",
    "\n",
    "# The number of samples you want to take:\n",
    "num_samples = 500\n",
    "# Here is how you can sample from Z:\n",
    "z_samples = np.random.randn(num_samples, 2)\n",
    "# Transforms these to samples of X \n",
    "x_samples = mu + np.einsum('ij,nj->ni', A, z_samples)\n",
    "# If you have never seen an Einstein sum, you may want to read this: \n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.einsum.html?highlight=einsum#numpy.einsum\n",
    "# Ridiculously useful... \n",
    "\n",
    "# Visualize everything\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# The contours\n",
    "# Points along x1 dimension\n",
    "x1 = np.linspace(-3, 5, 64)\n",
    "# Points along x2 dimension\n",
    "x2 = np.linspace(-3, 5, 64)\n",
    "# Create grid\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "# Flattened values of grid points\n",
    "X_flat = np.hstack([X1.flatten()[:, None], X2.flatten()[:, None]])\n",
    "# PDF values\n",
    "pdf_X_flat = X.pdf(X_flat).reshape(X1.shape)\n",
    "# Plot contours\n",
    "c = ax.contour(X1, X2, pdf_X_flat)\n",
    "# Plot the samples\n",
    "ax.plot(x_samples[:, 0], x_samples[:, 1], '.', markersize=2)\n",
    "# Plot the mean\n",
    "ax.plot(X.mean[0], X.mean[1], 'ro');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and that's how we sample from a multivariate Gaussian.\n",
    "\n",
    "## Questions\n",
    "\n",
    "+ Rerun the steps above after moving $\\boldsymbol{\\mu}$ to $(0, 1)$. Observe how the contours of the PDF move.\n",
    "+ Rerun the steps above for $\\Sigma_{12} = \\Sigma_{21} = 0.1$. Observe how the contours of the PDF change.\n",
    "+ Rerun the steps above for $\\Sigma_{12} = \\Sigma_{21} = -0.9$. Observe how the contours of the PDF change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
