{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 15 - Deep Neural Networks for Classification\n",
    "\n",
    "**THIS IS NOT FINAL**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ to introduce Deep Neural Networks as function approximators. \n",
    "+ to set up and train DNNs in `PyTorch`.\n",
    "+ to introduce Physics-informed Neural Networks (PINNs) and solve a few simple PDEs. \n",
    "\n",
    "## References \n",
    "\n",
    "+ Stanford CS231n course notes and video lectures (https://cs231n.github.io/). \n",
    "\n",
    "+ Physics-informed neural networks (Raissi et al, 2017) - Part I - https://arxiv.org/abs/1711.10561, Part II - https://arxiv.org/abs/1711.10566."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import six\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - generalized linear models (GLMs)\n",
    "\n",
    "We introduced generalized linear models as parametric function approximations earlier in course. Without loss of generality, the GLM approximation of a scalar function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ can be expressed as:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\phi(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "where, $\\phi(\\mathbf{x}) = (\\phi_1(\\mathbf{x}), \\phi_2(\\mathbf{x}), \\dots, \\phi_M(\\mathbf{x}))^T$ is a vector of basis functions and $\\mathbf{w}$ is a vector of weights. The regression task is the problem of inferring the parameters $\\mathbf{w}$ from a given dataset. To summarize, given a dataset of observations, $\\mathcal{D}$, our goal is to estimate the conditional probability - $p(\\mathbf{w}| \\mathcal{D})$. By Bayes' rule, $p(\\mathbf{w}| \\mathcal{D}) \\propto p(\\mathbf{w}) p(\\mathcal{D}|\\mathbf{w})$. \n",
    "The simplest way to do this is to approximate $p(\\mathbf{w}| \\mathcal{D})$ as a delta function centered at the mode of the posterior - leading to the classic maximum a-posteriori (MAP) estimate of $\\mathbf{w}$. \n",
    "If the prior is a Gaussian and the likelihood is also a Gaussian with fixed noise, the posterior over $\\mathbf{w}$ and the resulting predictive distribution on new test observations can both be derived in closed form. If these requirements are not satisfied we can use Markov Chain Monte Carlo (MCMC) or Variational Inference (VI) to approximate the posterior over $\\mathbf{w}$.\n",
    "\n",
    "In theory GLMs can approximate functions of any arbitrary complexity upto any arbitrary degree of tolerance. In practice one might encounter some difficulties:\n",
    "\n",
    "1. The choice of basis functions (what type and how many) is non-trivial. \n",
    "2. The number of basis functions required, when $\\mathrm{dim}(\\mathcal{X}) \\gg 1$ will explode (curse of dimensionality). \n",
    "3. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks (DNNs)\n",
    "\n",
    "Deep Neural Networks are function approximators that express information in a hierarchical or layered fashion. Mathematically, deep neural networks can be expressed as:\n",
    "$$\n",
    "f(\\mathbf{x}) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1( \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "In it's simplest setting, the layers $f_i$s are a composition of an elementwise nonlinearity with a linear transformation:\n",
    "$$\n",
    "f_i ( \\mathbf{z} ) = \\sigma_i ( \\mathbf{W}_i^{T} \\mathbf{z} + \\mathbf{b}_i  ),\n",
    "$$\n",
    "\n",
    "where, $\\mathbf{W}_i \\in \\mathbb{R}^{d_{z} \\times  d_i}, \\mathbf{b}_i \\in \\mathbb{R}^{d_i}$, where, $d_z = \\mathrm{dim}(\\mathbf{z})$, the dimensionality of the inputs to the $f_i$, $d_i$ is the dimensionality of the output from $f_i$ and $\\sigma_i$ is the elementwise nonlinearity. A function with this structure is called a *fully-connected* network. \n",
    "\n",
    "In deep learning parlance the matrix $\\mathbf{W_i}$ is referred to as a *weight* matrix, the vector $\\mathbf{b}_i$ is referred to as a *bias* and the function $\\sigma_i(\\cdot)$ is referred to as the *activation* function. It is typical for all but last layer of a DNN to have the same activation function. \n",
    "\n",
    "At the final layer, the dimensionality of the output $d_L$ and the activation function at the final layer, $\\sigma_L$ is dictated by constraints on the final output of the function $f$. For example:\n",
    "1. If the output from $f$ is a real number with no constraints, $d_L=1$ and $\\sigma_L(x) = 1$.\n",
    "2. If the output from $f$ is a positive real, $d_L = 1$ and $\\sigma_L(x) = \\exp(x)$. \n",
    "3. If the output from $f$ is a probability mass function on $K$ categories, $d_L = K$ and $\\sigma_L (x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{K} \\exp(x_j)}, i=1, 2, \\dots, K$.\n",
    "4. etc.\n",
    "\n",
    "Different ways of constructing the compositional structure of $f$ lead to different *architectures* such as fully connected networks (shown above), *recurrent neural networks*, *convolutional neural networks*, *autoencoders*, *residual networks* etc. etc. \n",
    "\n",
    "###  Activation functions \n",
    "\n",
    "The most common activation functions include the rectified Linear Units or ReLU (and variants), sigmoid functions, hyperolic tangents, sinusoids, step functions etc. A useful way to think about activation functions is that they are analogous to the effect of covariance functions on the regularity of Gaussian process sample paths - different choices of activation functions impose different regularity conditions on the sample paths of a DNN. \n",
    "\n",
    "See for instance, sample paths from a 2 hidden layer NN with different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda x : np.maximum(x, 0.)\n",
    "softsign = lambda x : x / (1. + np.abs(x))\n",
    "identity = lambda x : x\n",
    "\n",
    "def NN(x, fn):\n",
    "    D = [1, 50, 50, 1]\n",
    "    Ws = [np.random.randn(D[i-1], D[i]) for i in range(1, len(D))]\n",
    "    bs = [np.random.randn(D[i]) for i in range(1, len(D))]\n",
    "    y = x\n",
    "    for i in range(len(Ws) - 1):\n",
    "        y = fn(np.dot(y, Ws[i]) + bs[i])\n",
    "    y = np.dot(y, Ws[-1]) + bs[-1]\n",
    "    return y\n",
    "\n",
    "x = np.linspace(-1., 1, 100)[:, None]\n",
    "fig, ax = plt.subplots(2, 2, figsize = (12, 8))\n",
    "\n",
    "fns = [relu, softsign, np.tanh, np.sin] \n",
    "names = ['ReLU', 'Softsign', 'Hyperbolic Tangent', 'Sine']\n",
    "continuities = ['$C^0$', '$C^1$', '$C^{\\infty}$', '$C^{\\infty}$']\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        idx = 2*i + j\n",
    "        name = names[idx]\n",
    "        fn = fns[idx]\n",
    "        for k in range(5):\n",
    "            ax[i, j].plot(x, NN(x, fn))\n",
    "        ax[i, j].set_title(name+' ('+ continuities[idx] +')', fontsize=15)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic formulation\n",
    "\n",
    "\n",
    "The probabilistic formulation of DNNs follows the same approach as GLMs. We begin with a prior (say, zero-mean Gaussian with diagonal precision) on the unknown parameters (weights and biases):\n",
    "$$\n",
    "p(\\mathbf{W}_i) = \\mathcal{N}(\\mathbf{W}_i | 0, \\sigma_{W_i}^{2} \\mathbf{I}), \n",
    "$$\n",
    "and \n",
    "$$\n",
    "p(\\mathbf{b}_i) = \\mathcal{N}(\\mathbf{b}_i | 0, \\sigma_{b_i}^{2} \\mathbf{I}). \n",
    "$$\n",
    "\n",
    "Denote the flattened vector of all weights and biases collectively as $\\theta = (\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2, \\dots)$\n",
    "\n",
    "The structure of the data will inform our likelihood model. For the standard regression problem we get:\n",
    "$$\n",
    "p(\\mathbf{y}_i | \\theta ,\\mathbf{x}_i) =  \\mathcal{N} (\\mathbf{y}_i | f(\\mathbf{x}; \\theta), \\sigma^2).\n",
    "$$\n",
    "The data samples are assumed to be independent and identically distributed observations following the above likelihood model. \n",
    "\n",
    "\n",
    "The goal of inference is to then estimate the posterior distribution over the unknown weights and biases:\n",
    "$$\n",
    "p( \\theta |\\mathbf{y} ,  \\mathbf{X})  \\propto p(\\theta) p(\\mathbf{y} | \\theta , \\mathbf{X}) = p(\\theta) \\prod_{i=1}^{N} p(\\mathbf{y}_i | \\theta ,\\mathbf{x}_i).\n",
    "$$\n",
    "\n",
    "Note that the mean of the likelihood, $\\mathbb{E}[y_i | \\mathbf{x}_i, \\theta] = f(\\mathbf{x}_i ; \\theta)$ is a nonlinear function of $\\theta$. This makes posterior inference over $\\theta$ is therefore intractable in closed form. If the network has no hidden layers, $f(\\mathbf{x}; \\theta)$. The standard GLM is the special case of the DNN when it has no hidden layers, thereby making Bayesian inference analytically tractable.\n",
    "\n",
    "The most common approach to inferring the parameters of a DNN is to maximize the log of the posterior, i.e., we need to solve the optimization problem:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\mathrm{argmax}} \\log p( \\theta |\\mathbf{y} ,  \\mathbf{X})\n",
    "$$\n",
    "\n",
    "Assuming a flat prior over the biases and a independent Gaussian prior over all the weights (i.e., all the weight components have the same prior precision, say, $\\lambda$), we obtain the following optimization problem for DNN regression:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\mathrm{argmin}} \\sum_{i=1}^{N} \\frac{1}{2}(y_i - f(\\mathbf{x}_i, \\theta))^2   + \\lambda \\| \\mathbf{W} \\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "You probably notice that this looks like the standard DNN loss function (upto a multiplicative constant) with the $L_2$ regularization over the network weights. \n",
    "\n",
    "In general, you will find that DNN loss functions have the structure: \n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathbf{X}, \\mathbf{y}) = \\mathcal{D}(\\mathbf{y}, f(\\mathbf{X};\\theta)) + \\mathcal{R}(\\theta),\n",
    "$$\n",
    "\n",
    "where, $\\mathcal{D}(\\mathbf{y}, f(\\mathbf{X};\\theta))$ is a measure of the discrepancy between the DNN predictions and the data, and $\\mathcal{R}(\\theta)$ is a regularization term which corresponds to a prior over the DNN parameters and imposes specific inductive biases onto the network. For instance, selecting the Laplacian, rather than Gaussian prior, over the NN weights induces sparsity (i.e. collapses many of the DNN MAP weights to zero). \n",
    "\n",
    "For a more robust Bayesian approach to inference in DNNs, you can resort to MCMC (most commonly performed with the NUTS algorithm) or Variational Inference (typically with a mean-field Gaussian variational posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "There is one more ingredient that allows us to scale inference over datasets with millions of observation, namely, *stochastic gradient descent* or SGD. \n",
    "\n",
    "SGD attempts to solve stochastic optimization problems of the form: \n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\mathrm{argmin}} \\mathbb{E}_{\\xi \\sim p(\\xi)} [ \\mathcal{L}( \\theta ; \\xi) ]. \n",
    "$$\n",
    "where, the objective function is an expectation of a function of $\\theta$ over the random variable $\\xi$.\n",
    "SGD is an iterative method that starts with an initial estimate of the optimization variable $\\theta$, say $\\theta_0$, and updates it according to the rule: \n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - \\eta_i  \\frac{1}{M} \\sum_{j=1}^{M}\\nabla_{\\theta} \\mathcal{L}( \\theta_{i-1} ; \\xi_j),\\ i=1, 2, \\dots, \n",
    "$$\n",
    "where, $\\eta_i$ is the step-size in the $i^{th}$ iteration and $\\nabla_{\\theta}(\\cdot)$ is the gradient wrt to $\\theta$. \n",
    "The objective function is approximated with a Monte Carlo (MC) approximation. This update rule is guaranteed to converge to a local minimum of the objective function if the following conditions hold:\n",
    "$$\n",
    "\\underset{i \\rightarrow \\infty}{\\mathrm{lim}} \\eta_i = 0, \\\\\n",
    "\\sum_{i=1}^{\\infty} \\eta_i = \\infty, \\\\\n",
    "\\sum_{i=1}^{\\infty} \\eta_i^2 < \\infty.\n",
    "$$\n",
    "\n",
    "In training DNNs, the data plays the role of the random variable $\\xi$. Suppose we postulate that the data samples are drawn from some underlying distribution $(\\mathbf{x}_i, y_i) = d_i \\sim p(\\mathcal{D})$. The DNN training objective function can be expressed as the expectation of a function $\\mathcal{L}(\\theta)$ (the discrepancy + the regularizing term) over the entire dataset:\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\mathrm{argmin}} \\mathbb{E}_{\\mathcal{D} \\sim p(\\mathcal{D})} [ \\mathcal{L}( \\theta ; \\mathcal{D})]. \n",
    "$$\n",
    "The expectation can be approximated with an average (i.e. Monte Carlo approximation) over a small subset of the data samples, $\\tilde{\\mathcal{D}} = {(\\mathbf{x}_i, y_i)}_{i=1}^{M} \\subset \\mathcal{D}$:\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\mathrm{argmin}} \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}( \\theta ; \\mathbf{x}_i, y_i),\\ (\\mathbf{x}_i, y_i)  \\in \\tilde{\\mathcal{D}}.\n",
    "$$\n",
    "\n",
    "Thus, at each iteration of SGD, we only need to access and perform computations over a small subset $\\tilde{\\mathcal{D}}$ of the data instead of the full dataset $\\mathcal{D}$ which contains potentially millions of data samples. In neural network parlance, the subset $\\tilde{\\mathcal{D}}$ is called a *mini-batch*.\n",
    "\n",
    "To summarize, the training procedure goes as follows:\n",
    "\n",
    "1. Initialize all the parameters of the network $\\theta \\leftarrow \\theta_0$. \n",
    "\n",
    "2. For i = 1, 2, ..., :\n",
    "    \n",
    "    (i) Sample a subset of the dataset $\\tilde{\\mathcal{D}} \\sim p(\\mathcal{D})$. \n",
    "  \n",
    "    (ii) Compute the average loss over the minibatch $L_i = \\frac{1}{M} \\sum_{j=1}^{M} \\mathcal{L}( \\theta ; \\mathbf{x}_j, y_j)$.\n",
    "    \n",
    "    (iii) Update the weights of the network with the SGD update rule, $\\theta_i  \\leftarrow \\theta_{i-1} - \\eta_i L_i$.\n",
    "\n",
    "3. Stop when convergence criteria is met.\n",
    "\n",
    "In practice, vanilla SGD is rarely used due to numerical instabilities associated with it. There are many variants of SGD which apply preconditioning on the gradient updates to accelerate convergence to a local minimum. The most commonly used variant is the Adaptive Moments (ADAM) optimization method ([see here](https://arxiv.org/pdf/1412.6980.pdf)). \n",
    "\n",
    "## Computing gradients \n",
    "\n",
    "To be able to use SGD to train our DNNs we need to able to obtain gradients of the DNN wrt $\\theta$. As the size and structure of the network increases, deriving gradients by hand will become extremely tedious (practically impossible). Fortunately, we need not ever have to derive gradients by hand. Software libraries such as `PyTorch` use automatic differentiation to derive the gradients. Note automatic differentiation returns *exact* gradients (in contrast to the approximation of the gradient one might obtain from finite differences). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a network in `PyTorch`\n",
    "\n",
    "`PyTorch` is a library for fast numerical linear algebra and statistical computations (just like `numpy`). It comes with built-in support for GPU acceleration and automatic differentiation. It also shares a high-degree of syntactic similarity with `numpy`. Along with with `tensorflow`, it is one of the two most popular frameworks for implementing deep learning methods.\n",
    "\n",
    "`PyTorch` allows you to implement arbitrarily complex and large DNN by working with templates and utilities defined in `torch.nn`. The main class you need to know is the `torch.nn.Module` class - it serves as a convenient wrapper for defining DNNs with any arbitrary structure.\n",
    "Trainable parameters in `PyTorch`  are defined as instances of `nn.Parameter` class. The `nn.Module` class registers every instance of a parameter definition inside it's `__init__` function. \n",
    "\n",
    "Here's a basic template to implement an $L$-layer fully connected network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of a fully connected feed forward \n",
    "    Neural network in pytorch. \n",
    "    \"\"\"\n",
    "    def __init__(self, layersizes=[1, 1], \n",
    "                 activation=torch.relu,\n",
    "                 final_layer_activation=None):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            layersizes <list/tuple>: An iterable ordered object containing\n",
    "                                 the sizes of the tensors from the\n",
    "                                 input layer to the final output. \n",
    "                                 (See example below). \n",
    "            activation <callable>: A python callable through which\n",
    "                                    torch backpropagation is possible.\n",
    "            final_layer_activation <callable>: A python callable for \n",
    "                                    the final layer activation function.\n",
    "                                    Default: None (for regression problems)\n",
    "        \n",
    "        EXAMPLE: \n",
    "            To define a NN with an input of size 2, 2 hidden layers of size \n",
    "            50 and 50, output of size 1, with tanh activation function: \n",
    "            >> layers = [2, 50, 50, 1]\n",
    "            >> neuralnet = NeuralNet(layers, activation=torch.tanh)\n",
    "            >> x = torch.randn(100, 2)   # 100 randomly sampled inputs \n",
    "            >> output = neuralnet(x)  # compute the prediction at x.\n",
    "        \n",
    "        Inheriting from nn.Module ensures that all\n",
    "        NN layers defined within the __init__ function are captured and \n",
    "        stored in an OrderedDict object for easy accesability.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layersizes = layersizes\n",
    "        self.input_dim = self.layersizes[0]\n",
    "        self.hidden_sizes = self.layersizes[1:-1]\n",
    "        self.output_dim = self.layersizes[-1]\n",
    "        self.activation = activation\n",
    "        self.final_layer_activation = final_layer_activation\n",
    "        if self.final_layer_activation is None:\n",
    "            self.final_layer_activation = nn.Identity()\n",
    "        self.nlayers = len(self.hidden_sizes) + 1\n",
    "        self.layernames = [] ## Dictionary to store all the FC layers \n",
    "        \n",
    "        # define FC layers\n",
    "        for i in range(self.nlayers):\n",
    "            layername = 'fc_{}'.format(i+1)\n",
    "            layermodule = nn.Linear(self.layersizes[i], self.layersizes[i+1])\n",
    "            self.layernames.append(layername)\n",
    "            setattr(self, layername, layermodule)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement the forward pass of the NN. \n",
    "        \"\"\"\n",
    "        for i, layername in enumerate(self.layernames):\n",
    "            fclayer = getattr(self, layername)\n",
    "            x = fclayer(x)\n",
    "            if i == self.nlayers - 1:\n",
    "                x = self.final_layer_activation(x)\n",
    "            else:\n",
    "                x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# # sanity check \n",
    "# net = NeuralNetwork([1, 40, 40, 40, 1], torch.tanh)\n",
    "# x = torch.linspace(0., 1., 100)[:, None]\n",
    "# for m in net.modules():\n",
    "#     print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing parameters of a network in `PyTorch`\n",
    "\n",
    "Classes for all of the common SGD routines are implemented in `torch.optim`. To use a particular optimization method, you need to define an instance of it and initialize it with a iterable (list/tuple/generator) consisting of the parameters that need to be optimized and a starting step size (or learning rate). The list of parameters of any instance of a torch `Module` can be obtained with `module.parameters()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Motor Cycle Data\n",
    "\n",
    "Here's a demonstration of the process of setting up and training a simple fully-connected neural network. We will use the motor cycle accident dataset for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('motor.dat')\n",
    "X = data[:, 0][:, None]\n",
    "Y = data[:, 1][:, None]\n",
    "N = len(X)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = int(0.7*N)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, train_size=Ntrain)\n",
    "\n",
    "# scale the data\n",
    "xscaler = StandardScaler()\n",
    "yscaler = StandardScaler()\n",
    "Xtrain=xscaler.fit_transform(Xtrain)\n",
    "Ytrain=yscaler.fit_transform(Ytrain)\n",
    "Xval, Yval = xscaler.transform(Xval), yscaler.transform(Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate the network \n",
    "layersizes = [1, 20, 20, 1]\n",
    "activation = torch.tanh\n",
    "net = NeuralNetwork(layersizes=layersizes, \n",
    "                    activation=activation)\n",
    "\n",
    "# define a misfit function\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "## L2 regularization\n",
    "lmbda = 1e-3 # reg. constant \n",
    "def L2loss(lmbda, net):\n",
    "    reg = torch.tensor(0.)\n",
    "    for m in net.modules():\n",
    "        if hasattr(m, 'weight'):\n",
    "            reg += m.weight.norm()**2\n",
    "    return lmbda*reg\n",
    "\n",
    "### define an optimizer \n",
    "lr = 1e-2\n",
    "update = torch.optim.Adam(params=net.parameters(), lr=lr)\n",
    "\n",
    "# training loop params \n",
    "maxiter = 5000\n",
    "batchsize = 32\n",
    "Xtrain, Ytrain = torch.Tensor(Xtrain), torch.Tensor(Ytrain)\n",
    "Xval, Yval = torch.Tensor(Xval), torch.Tensor(Yval)\n",
    "best_stat_dict = net.state_dict()\n",
    "best_val_mse_loss = np.inf\n",
    "\n",
    "## training loop \n",
    "for i in range(maxiter):\n",
    "    # sample a batch of data \n",
    "    batchidx = np.random.randint(0, len(Xtrain), batchsize)\n",
    "    xbatch, ybatch = Xtrain[batchidx], Ytrain[batchidx]\n",
    "    \n",
    "    # zero out the gradient buffers\n",
    "    update.zero_grad()\n",
    "    \n",
    "    #forward prop \n",
    "    ypred = net(xbatch)\n",
    "    \n",
    "    # compute the regularized loss \n",
    "    loss = mse(ypred, ybatch) + L2loss(lmbda, net)\n",
    "    \n",
    "    # back prop to get gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # take one optimization step\n",
    "    update.step()\n",
    "    \n",
    "    # print progress \n",
    "    if (i+1)%100 == 0:\n",
    "        val_mse_loss = mse(Yval, net(Xval))\n",
    "        print(' [Iteration %4d] Validation MSE loss: %.3f'%(i+1, val_mse_loss))\n",
    "        if val_mse_loss < best_val_mse_loss:\n",
    "            best_state_dict = net.state_dict()\n",
    "net.load_state_dict(best_stat_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xtest = torch.Tensor(xscaler.transform(np.linspace(0., 60., 1000)[:, None]))\n",
    "Ypred = net(Xtest)\n",
    "\n",
    "plt.figure( figsize=(10, 8) )\n",
    "plt.plot(xscaler.inverse_transform(Xtest.data.numpy()), \n",
    "         yscaler.inverse_transform(Ypred.data.numpy()),\n",
    "         linewidth=3,\n",
    "         label='Test Predictions')\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "plt.legend(loc='best', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-exhaustive list of topics not covered here:\n",
    "\n",
    "1. Regularizing a neural network with techniques such as dropout (`nn.Dropout`), batch normalization (`nn.BatchNorm1D`, `nn.BatchNorm2D`, `nn.BatchNorm3D`).\n",
    "\n",
    "2. Weight Initialization (uniform, normal, [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), [He initialization](https://arxiv.org/pdf/1502.01852.pdf) etc.) All common initialization techniques can be found under `nn.init` ([see here](https://pytorch.org/docs/stable/nn.init.html#nn-init-doc)).\n",
    "\n",
    "3. Learning rate (or step size) schedule (Step schedule, exponential schedule, cosine annealing etc.) ([look for the section on how to adjust learning rate here](https://pytorch.org/docs/stable/optim.html)).\n",
    "\n",
    "4. Hyperparameter optimization (grid search, cross-validation, Bayesian optimization etc.)\n",
    "\n",
    "5. Model averaging. \n",
    "\n",
    "6. Stochastic weight averaging - https://arxiv.org/abs/1803.05407. This feature is available in `PyTorch`; [see here](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/?utm_source=social-facebook&utm_medium=PyTorch&utm_campaign=organic&utm_content=post-url&utm_offering=artificial-intelligence&utm_product=AIStochasticWeight_050819&fbclid=IwAR28pSPYvd-9Qlx2nOvZF7c965TcnvNQENytm1HKQy1vKLHNifEZRzp5S8s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Network \n",
    "\n",
    "Starting with a prior $p(\\theta)$ on the DNN weights and biases and a likelihood model $p(\\mathcal{D} | \\theta)$, we wish to approximate the posterior distribution over the DNN weights and biases $p(\\theta | \\mathcal{D})$. \n",
    "\n",
    "To efficiently implement and train a Bayesian Neural Network, we will use the `pyro` probabilistic programming library which is written on top of `PyTorch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist \n",
    "from pyro import infer\n",
    "from pyro import optim \n",
    "from pyro.contrib import autoguide\n",
    "from pyro.nn import PyroSample, PyroModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralNetwork( PyroModule ):\n",
    "    def __init__(self, layersizes=[1, 1], activation = torch.tanh,\n",
    "                 final_layer_activation = None, \n",
    "                 prior_scale = 0.1, \n",
    "                 sigma = 0.1):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            layersizes <list/tuple>: An iterable ordered object containing\n",
    "                                 the sizes of the tensors from the\n",
    "                                 input layer to the final output. \n",
    "                                 (See example below). \n",
    "            activation <callable>: A python callable through which\n",
    "                                    torch backpropagation is possible.\n",
    "            final_layer_activation <callable>: A python callable for \n",
    "                                    the final layer activation function.\n",
    "                                    Default: None (for regression problems)\n",
    "            prior_scale <float>: The prior standard deviation over the weights. \n",
    "            sigma <float>: The likelihood noise standard deviation\n",
    "        \"\"\"\n",
    "        super(BayesianNeuralNetwork, self).__init__()\n",
    "        self.prior_scale = prior_scale\n",
    "        self.sigma       = sigma\n",
    "        self.layersizes = layersizes\n",
    "        self.input_dim = self.layersizes[0]\n",
    "        self.hidden_sizes = self.layersizes[1:-1]\n",
    "        self.output_dim = self.layersizes[-1]\n",
    "        self.activation = activation\n",
    "        self.final_layer_activation = final_layer_activation\n",
    "        if self.final_layer_activation is None:\n",
    "            self.final_layer_activation = nn.Identity()\n",
    "        self.nlayers = len(self.hidden_sizes) + 1\n",
    "        self.layernames = [] \n",
    "        \n",
    "        # define FC layers\n",
    "        # to treat the parameters of the network as latent variables\n",
    "        # place priors over them using PyroSample \n",
    "        for i in range(self.nlayers):\n",
    "            layername = 'fc_{}'.format(i+1)\n",
    "            self.layernames.append(layername)\n",
    "            layer = PyroModule[nn.Linear](self.layersizes[i], self.layersizes[i+1])\n",
    "            weightprior = dist.Normal(0., prior_scale).expand(layer.weight.shape).to_event(2)\n",
    "            biasprior = dist.Normal(0., prior_scale).expand(layer.bias.shape).to_event(1)\n",
    "            layer.weight = PyroSample(weightprior)\n",
    "            layer.bias = PyroSample(biasprior)\n",
    "            setattr(self, layername, layer)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"\n",
    "        Specify the generative process here. \n",
    "        \n",
    "        INPUTS:\n",
    "            x -> Input features on which to condition the observations. \n",
    "            y -> Observed data. \n",
    "            \n",
    "        RETURNS:\n",
    "            loc -> Predictive mean.\n",
    "        \"\"\"\n",
    "        ysigma = self.sigma \n",
    "        \n",
    "        # get the mean \n",
    "        ymean = x\n",
    "        for i, layername in enumerate(self.layernames):\n",
    "            fclayer = getattr(self, layername)\n",
    "            ymean = fclayer(ymean)\n",
    "            if i == self.nlayers - 1:\n",
    "                ymean = self.final_layer_activation( ymean )\n",
    "            else:\n",
    "                ymean = self.activation( ymean )\n",
    "        ymean = ymean[:,0]\n",
    "                \n",
    "        # pyro.plate will take care of mini batch scaling \n",
    "        with pyro.plate('data', x.shape[0]):\n",
    "            y = pyro.sample('y', dist.Normal(ymean, ysigma), obs=y)\n",
    "        \n",
    "        return ymean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Bayesian Neural network \n",
    "prior_scale = 1.\n",
    "sigma = 1e-1\n",
    "bnn = BayesianNeuralNetwork(layersizes=[1, 200, 1], \n",
    "                            prior_scale=prior_scale,\n",
    "                            sigma=1e-1)   # this will be the pyro `model` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyro` has several choices for the variational posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[name for name in dir(infer.autoguide) if 'Auto' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training validation splot and preprocessing \n",
    "Ntrain = int(0.7*N)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, train_size=Ntrain)\n",
    "\n",
    "# scale the data\n",
    "xscaler = StandardScaler()\n",
    "yscaler = StandardScaler()\n",
    "Xtrain=xscaler.fit_transform(Xtrain)\n",
    "Ytrain=yscaler.fit_transform(Ytrain)\n",
    "Xval, Yval = xscaler.transform(Xval), yscaler.transform(Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the variational posterior \n",
    "q_bnn = infer.autoguide.AutoDiagonalNormal( bnn )\n",
    "\n",
    "# always clear the parameter store before inference\n",
    "pyro.clear_param_store()  \n",
    "\n",
    "# define a loss function \n",
    "loss = infer.trace_elbo.Trace_ELBO(num_particles=5)\n",
    "\n",
    "# define an optimizer \n",
    "optim_dict = {'lr':1e-2}  # dictionary of optimizer settings\n",
    "optimizer = pyro.optim.Adam(optim_dict)\n",
    "\n",
    "# instantiate the inference engine \n",
    "svi = infer.SVI(model=bnn, \n",
    "                guide=q_bnn, \n",
    "                optim=optimizer, \n",
    "                loss=loss)\n",
    "\n",
    "### TRAINING LOOP ###\n",
    "niters = 5000\n",
    "losses = []\n",
    "Xtrain, Ytrain = torch.Tensor(Xtrain), torch.Tensor(Ytrain)\n",
    "Xval, Yval = torch.Tensor(Xval), torch.Tensor(Yval)\n",
    "data = torch.Tensor(Y)\n",
    "for i in tqdm_notebook(range(niters)):\n",
    "    loss = svi.step(Xtrain, Ytrain[:,0])\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "losses = np.array(losses)\n",
    "plt.plot(np.arange(1, niters+1), -losses)\n",
    "plt.ylabel('ELBO', fontsize=15)\n",
    "plt.xlabel('Iteration', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the posterior predictive samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = torch.Tensor(xscaler.transform(np.linspace(0., 60., 500)[:, None]))\n",
    "predictor = infer.Predictive(model=bnn, \n",
    "                             guide=q_bnn, \n",
    "                             num_samples=1000, \n",
    "                             return_sites=(\"y\", \"_RETURN\"))\n",
    "ppsamples = predictor(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppmean = ppsamples[\"_RETURN\"].data.numpy().mean(0)\n",
    "ppmean = yscaler.inverse_transform(ppmean)\n",
    "pplow, pphigh = np.percentile(ppsamples[\"_RETURN\"].data.numpy(), axis=0, q=[2.5, 97.5])\n",
    "pplow = yscaler.inverse_transform(pplow)\n",
    "pphigh = yscaler.inverse_transform(pphigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(10, 8) )\n",
    "Xtest=xscaler.inverse_transform(Xtest.data.numpy())\n",
    "plt.plot(Xtest, \n",
    "         ppmean,\n",
    "         linewidth=3,\n",
    "         label='Test Predictions')\n",
    "plt.fill_between(Xtest[:,0], pplow, pphigh, alpha=0.25, label='95% interval')\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "\n",
    "plt.legend(loc='best', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational challenges of training BNNs\n",
    "\n",
    "Training DNNs in a truly Bayesian way is challenging for a number of reasons:\n",
    "\n",
    "1. Since the likelihood mean is nonlinear, the model evidence is intractable in closed form - we cannot leverage conjugacy. \n",
    "\n",
    "2. A DNN is typically highly overparameterized. This means we have to perform approximate inference in a very high-dimensional space; there is a non-trivial space cost in addition to time cost for Bayesian training of DNNs. For example, say a DNN has $N$ weights and biases in total. \n",
    "\n",
    "    (i) The performance of Metropolis-Hastings degrades rapidly with the number of parameters, requiring tens of thousands (or more) simulations to reach equilibrium. In practice, one often finds that classical MH samplers simply do not converge. If the sampler does converge, and say after burning and thinning you retain 1000 samples from the MCMC trace, you need to store $1000N$ floating point numbers in memory to compute posterior expectations.\n",
    "    \n",
    "    (ii) The HMC/NUTS approach to inference is based on simulating a hypothetical Hamiltonian system where the vector of latent variables $\\theta$ are augmented with a vector of momentum variables $\\mathbf{r}$ of the same size. At any given iteration of HMC/NUTS you work with $2N$ variables in memory. \n",
    "    \n",
    "    (iii) The most cost efficient Variational Inference approximation is a mean field Gaussian $p(\\theta | \\mathcal{D}) \\approx q(\\theta) = \\prod_{i=1}^{N} q(\\theta_i)$, where, $q(\\theta_i) = \\mathcal{N}(\\theta_i | \\mu_i, \\sigma_i^2)$. Using this variational approximation you will capture one of the modes of the posterior and none of the correlation across parameters. To be able to compute expectations afterwards, you need to be able to store $2N$ parameters in memory (all the $\\mu_i$s and $\\sigma_i$s). \n",
    "    \n",
    "    (iv) A full-rank Gaussian variational posterior has memory requirements of $\\mathcal{O}(N + N^2)$ - which, for reasonably large DNNs, is not feasible. \n",
    "    \n",
    "    (v) A compromise variational approximation between the mean field Gaussian and the full rank Gaussian is to use a Gaussian whose covariance matrix is a diagonal with a low-rank update. Such a variational posterior has memory requirements of $\\mathcal{O}((2+r)N)$, where, $r$ is the rank of the update.\n",
    "    \n",
    "    (vi) VI, of any kind, is prone to getting trapped in a bad local minima of the negative ELBO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Deep Neural Networks \n",
    "\n",
    "\n",
    "Because DNNs afford a great deal of flexibility in their structure and training methodology, they find use in many non-traditional machine learning tasks. A recently emergent area of research in scientific machine learning is the application of DNNs to the task of partial differential equations (PDEs) (see the references). \n",
    "\n",
    "The idea is simple. Say we have a PDE of the form:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + \\mathcal{N}\\left[u, \\frac{\\partial u}{\\partial x}, \\dots \\right]=0, \\ \\forall \\mathbf{x} \\in \\Omega, \\ t \\in [0, T],\n",
    "$$\n",
    "\n",
    "subject to initial conditions $u(\\mathbf{x}, 0) = g(\\mathbf{x})$ and boundary conditions $u(\\mathbf{x}, t) = h(\\mathbf{x}, t)$. \n",
    "\n",
    "We start with a DNN trial solution, $u(x, t) = \\varphi(x, t; \\theta)$, where, $\\theta$ represents all the tunable parameters of the network. Suppose we sample a bunch of collocation points inside the PDE domain $\\{ \\mathbf{x}^{(i)} \\}_i \\subset \\Omega$ and a bunch of collocation points $\\{t^{(i)}\\}_i \\in (0, T]$. For $\\varphi$ to accurate represent the solution, the LHS of the PDE, evaluated at these collocation points, must be 0. We can therefore formulate a loss function of the form:\n",
    "$$\n",
    "\\mathcal{R}_{\\Omega} = \\frac{1}{N_{\\Omega}} \\sum_{i}\\left( \\frac{\\partial u}{\\partial t}\\Big|_{x=x^{(i)}, t=t^{(i)}} + \\mathcal{N}\\left[u, \\frac{\\partial u}{\\partial x}, \\dots \\right]\\Big|_{x=x^{(i)}, t=t^{(i)}} \\right)^2,\n",
    "$$\n",
    "where, $N_{\\Omega}$ is the number of collocation points.  This is the squared residual of the PDE at the collocation points. \n",
    "\n",
    "We can similarly take a set of collocation points in space and time, sampled arbitrarily, and formulate a boundary residual:\n",
    "$$\n",
    "\\mathcal{R}_{\\partial \\Omega} =\\frac{1}{N_{\\partial\\Omega}} \\sum_{i} h(\\mathbf{x}, t)^2\\Big|_{x=x^{(i)}, t=t^{(i)}},\n",
    "$$\n",
    "where, $N_{\\partial\\Omega}$ is the number of boundary collocation points. Similarly, the initial conditions residual will be:\n",
    "$$\n",
    "\\mathcal{R}_{t} =\\frac{1}{N_{t}} \\sum_{i} g(\\mathbf{x})^2\\Big|_{x=x^{(i)}}.\n",
    "$$\n",
    "\n",
    "We can take all the derivatives necessary to construct these residuals for free using automatic differentiation (`torch.autograd.grad` in `PyTorch`). All of these residual terms individually are functions of the parameters $\\theta$ of the DNN $\\varphi$ and we can compose a total residual loss function by adding them up:\n",
    "$$\n",
    "\\mathcal{R}(\\theta) = \\mathcal{R}_{ \\Omega}(\\theta) + \\mathcal{R}_{\\partial \\Omega}(\\theta) + \\mathcal{R}_{t}(\\theta).\n",
    "$$\n",
    "\n",
    "Now all we have to do is to use SGD to solve the optimization problem:\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\mathrm{argmin}}\\mathcal{R}(\\theta).\n",
    "$$\n",
    "\n",
    "Below we demonstrate this idea by applying it to the Burgers' equation in 1D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burgers' Equation in 1D\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\frac{0.01}{\\pi} \\frac{\\partial^2 u}{\\partial x}, \\ x \\in [-1, 1],\\ t \\in [0, 1],\n",
    "$$\n",
    "\n",
    "subject to initial conditions:\n",
    "$$\n",
    "u(0, x) = -\\sin \\pi x, \n",
    "$$\n",
    "and periodic boundary conditions:\n",
    "$$\n",
    "u(t, -1) = u(t, 1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqresidual(net, x, t):\n",
    "    assert x.requires_grad == True and t.requires_grad == True\n",
    "    X = torch.stack([x, t], 1).squeeze(-1)\n",
    "    u = net(X)\n",
    "    ux = grad(u, x, retain_graph=True, create_graph=True, grad_outputs=torch.ones_like(u))[0]\n",
    "    ut = grad(u, t, retain_graph=True, create_graph=True, grad_outputs=torch.ones_like(u))[0]\n",
    "    uxx = grad(ux, x, retain_graph=True, create_graph=True, grad_outputs=torch.ones_like(ux))[0]\n",
    "    residual = ut + u*ux - (0.01/np.pi)*uxx\n",
    "    return torch.norm(residual)**2\n",
    "\n",
    "def boundaryresidual(net, t):\n",
    "    leftx = -torch.ones_like(t, requires_grad=True)\n",
    "    leftX = torch.stack([leftx, t], 1).squeeze(-1)\n",
    "    \n",
    "    rightx = torch.ones_like(t, requires_grad=True)\n",
    "    rightX = torch.stack([leftx, t], 1).squeeze(-1)\n",
    "    \n",
    "    leftU = net(leftX)\n",
    "    rightU = net(rightX)\n",
    "    boundary_res = torch.norm(leftU-rightU)**2\n",
    "    \n",
    "    return boundary_res\n",
    "    \n",
    "    \n",
    "def initialcond_residual(net, x):\n",
    "    initt = torch.zeros_like(x, requires_grad=True)\n",
    "    initX = torch.stack([x, initt], 1).squeeze(-1)\n",
    "    \n",
    "    initU = net(initX)\n",
    "    init_res = torch.norm(initU + torch.sin(np.pi*x))**2\n",
    "    return init_res\n",
    "\n",
    "def L2loss(lmbda, net):\n",
    "    reg = torch.tensor(0.)\n",
    "    for m in net.modules():\n",
    "        if hasattr(m, 'weight'):\n",
    "            reg += m.weight.norm()**2\n",
    "    return lmbda*reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## define a network to approximate the PDE solution \n",
    "layersizes = [2] + [20]*9 + [1]  \n",
    "pinn = NeuralNetwork(layersizes=layersizes, activation=torch.sin)\n",
    "\n",
    "### define an optimizer \n",
    "lr = 1e-3\n",
    "update = torch.optim.Adam(params=pinn.parameters(), lr=lr)\n",
    "\n",
    "# training loop params \n",
    "maxiter = 5000\n",
    "batchsize = 500\n",
    "lmbda = 1e-5\n",
    "best_stat_dict = pinn.state_dict()\n",
    "best_val_loss = np.inf\n",
    "tval = torch.rand((200,1), requires_grad=True)\n",
    "xval = -1. + 2*torch.rand((200,1), requires_grad=True)\n",
    "\n",
    "## training loop \n",
    "for i in range(maxiter):\n",
    "    # sample a batch of data \n",
    "    tbatch = torch.rand((batchsize,1), requires_grad=True)\n",
    "    xbatch = -1. + 2*torch.rand((batchsize,1), requires_grad=True)\n",
    "    \n",
    "    # zero out the gradient buffers\n",
    "    update.zero_grad()\n",
    "    \n",
    "    # compute the regularized loss \n",
    "    loss = sqresidual(pinn, xbatch, tbatch) +\\\n",
    "           boundaryresidual(pinn, tbatch) +\\\n",
    "           initialcond_residual(pinn, xbatch) #+ L2loss(lmbda, pinn)\n",
    "    \n",
    "    # back prop to get gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # take one optimization step\n",
    "    update.step()\n",
    "    \n",
    "    # print progress \n",
    "    if (i+1)%100 == 0:\n",
    "        val_loss = sqresidual(pinn, xval, tval)\n",
    "        print(' [Iteration %4d] Validation Squared Residual: %.5f'%(i+1, val_loss))\n",
    "        if val_loss < best_val_loss:\n",
    "            best_state_dict = pinn.state_dict()\n",
    "pinn.load_state_dict(best_stat_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = torch.linspace(-1., 1, 100)\n",
    "ttest = torch.linspace(0., 1, 100)\n",
    "Xtest1, Xtest2 = torch.meshgrid(xtest, ttest)\n",
    "Xtest = torch.stack([Xtest1.flatten(), Xtest2.flatten()], 1).squeeze(-1)\n",
    "Upred = pinn(Xtest).view(100, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(Xtest1.data.numpy(), Xtest2.data.numpy(), Upred.data.numpy().T, 100, cmap='plasma')\n",
    "plt.title('$u(x, t)$', fontsize=15)\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.colorbar()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
