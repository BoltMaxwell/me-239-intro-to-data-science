{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 9.2: Polynomial Regression\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce polynomial regression\n",
    "\n",
    "## An example where things work as expected\n",
    "\n",
    "We take up where we left in the previous hands-on activity.\n",
    "Let's try to fit a linear regression model to data generated from:\n",
    "$$\n",
    "y_i = -0.5 + 2x_i + 2x_i^2 + \\epsilon_i,\n",
    "$$\n",
    "where $\\epsilon_i \\sim N(0, 1)$ and where we sample $x_i \\sim U([-1,1])$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "w2_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + w2_true * x ** 2 + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw that the linear model does not work here.\n",
    "We need to try to fit a quadratic model:\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2,\n",
    "$$\n",
    "by minimizing the square loss:\n",
    "$$\n",
    "L(\\mathbf{w}) = \\sum_{i=1}^N(y_i - w_0 - w_1 x_i - w_2 x_i^2)^2 = \\parallel \\mathbf{y} - \\mathbf{X}\\mathbf{w}\\parallel_2^2,\n",
    "$$\n",
    "where $\\mathbf{y} = (y_1,\\dots,y_N)$ is the vector of observations, $\\mathbf{w} = (w_0, w_1)$ is the weight vector, and the $N\\times 2$ *design matrix* $\\mathbf{X}$ is:\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix} \n",
    "1 & x_1 & x_1^2\\\\\n",
    "1 & x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_N & x_N^2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "As before, to solve the least squares problems numerically, we need to for $\\mathbf{X}$.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((num_obs, 1)), x.reshape((num_obs, 1)), x.reshape((num_obs, 1)) ** 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It returns quite a few things that we haven't explained yet, which are going to ignore\n",
    "w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print('w_0 = {0:1.2f}'.format(w[0]))\n",
    "print('w_1 = {0:1.2f}'.format(w[1]))\n",
    "print('w_2 = {0:1.2f}'.format(w[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# The model we just fitted\n",
    "yy = w[0] + w[1] * xx + w[2] * xx ** 2\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "# overlay our prediction\n",
    "ax.plot(xx, yy, '--', label='Fitted model')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Repeat with very small ``num_obs`` and very large ``num_obs`` and observe the behavior of the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "What would have happened if we tried to use a higher degree polynomial.\n",
    "Let's write some code that can try to fit a polynomial of any degree.\n",
    "To achieve this, we need to be able to evaluate a design matrix of the form:\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix} \n",
    "1 & x_1 & x_1^2\\dots & x_1^\\rho\\\\\n",
    "1 & x_2 & x_2^2\\dots & x_2^\\rho\\\\\n",
    "\\vdots & \\vdots\\dots & \\vdots\\\\\n",
    "1 & x_N & x_N^2 \\dots & x_N^\\rho\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $\\rho$ is the degree of the polynomial.\n",
    "We need to write some code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting a degree 3 polynomial and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "# The design matrix is:\n",
    "X = get_polynomial_design_matrix(x[:, None], degree)\n",
    "# And we fit just like previously:\n",
    "w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print('w = ', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the fit.\n",
    "Notice, that for making predictions I am evaluating the design matrix on the points I want to make predictions at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# The model we just fitted\n",
    "XX = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "yy = np.dot(XX, w)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "# overlay our prediction\n",
    "ax.plot(xx, yy, '--', label='Fitted model')\n",
    "ax.set_title(r'$\\rho = {0:d}$'.format(degree))\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Start increasing the polynomial degree from 3, to 4, to a number where things get bad... You will soon start *overfitting*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
