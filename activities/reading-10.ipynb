{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 10 - Bayesian Linear Regression\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Understand the probabilistic interpretation of least squares\n",
    "+ Use regularization to avoid overfitting\n",
    "+ Use Bayesian regression to quantify epistemic uncertainty induced by limited data\n",
    "+ Introduce the concept of point-predictive distribution\n",
    "\n",
    "## Probabilistic regression I (maximum likelihood)\n",
    "\n",
    "This first version of probabilistic regression, maximum likelihood, is identical to least squares but gives you the ability to estimate the measurement noise.\n",
    "\n",
    "We wish to model the data using some **fixed** basis/features:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "However, instead of directly picking a loss function to minimize we come up with a probabilistic description of the measurement process.\n",
    "In particular, we *model the measurement process* using a **likelihood** function:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w} \\sim p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}).\n",
    "$$\n",
    "\n",
    "What is the interpretation of the likelihood function?\n",
    "Well, $p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w})$ tells us how plausible is it to observe $\\mathbf{y}_{1:n}$ at inputs $\\mathbf{x}_{1:n}$, if we know that the model parameters are $\\mathbf{w}$.\n",
    "\n",
    "Since, in almost all the cases we consider, the measurements are independent conditioned on the model, then likelihood of the data factorizes as follows:\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}) = \\prod_{i=1}^np(y_i|\\mathbf{x}_i, \\mathbf{w}),\n",
    "$$\n",
    "where $p(y_i|\\mathbf{x}_i,\\mathbf{w})$ is the likelihood of a single measurement.\n",
    "\n",
    "The most common choice for the likehood of a single measurement is to pick it to be Gaussian.\n",
    "We assign:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p(y_i|\\mathbf{x}_i, \\mathbf{w}, \\sigma) &=& \\mathcal{N}\\left(y_i| y(\\mathbf{x}_i;\\mathbf{w}), \\sigma^2\\right)\\\\\n",
    "&=& \\mathcal{N}\\left(y_i | \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)}, \\sigma^2\\right),\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\sigma$ models the **noise**.\n",
    "This correspond to the belief that our measurement is around the model prediction $\\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})}$\n",
    "but it is contaminated with Gaussian noice of variance $\\sigma^2$.\n",
    "\n",
    "Assuming a Gaussian likelihood for a single observation, we have for all the data:\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) = \\mathcal{N}\\left(\\mathbf{y}_{1:n} | \\mathbf{\\Phi}\\mathbf{w}, \\sigma^2\\mathbf{I}_n\\right).\n",
    "$$\n",
    "Let's look up the form of the multivariate Gaussian from the ([Wiki](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)):\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) \n",
    "= (2\\pi)^{-\\frac{n}{2}}\\sigma^{-n} e^{-\\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2}.\n",
    "$$\n",
    "\n",
    "### Maximum Likelihood Estimate of $\\mathbf{w}$\n",
    "\n",
    "Once we have a likelihood, we can train the model by maximizing the likelihood:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MLE}} = \\arg\\max_{\\mathbf{w}} p(\\mathbf{y}_{1:n}, |\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma).\n",
    "$$\n",
    "When we do this we are essentially selecting the model that makes the observations most likely.\n",
    "For the Gaussian likelihood, we have:\n",
    "$$\n",
    "\\log p(\\mathbf{y}_{1:n}, |\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) =\n",
    "-\\frac{n}{2}\\log(2\\pi)\n",
    "-n\\log\\sigma\n",
    "- \\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2.\n",
    "$$\n",
    "Taking the derivatives of this expression with respect to $\\mathbf{w}$ and setting them equal to zero (sufficient condition) yields the same solution as least squares.\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MLE}} \\equiv \\mathbf{w}_{\\mbox{LS}}.\n",
    "$$\n",
    "\n",
    "### Maximum Likelihood Estimate of $\\sigma$\n",
    "The probabilistic interpretation above gives the same solution as least squares.\n",
    "To start undersanding its power, notice that it can also give us an estimate for the measurement noise variance $\\sigma^2$.\n",
    "All you have to do is maximize likelihood with respect to $\\sigma$.\n",
    "For the Gaussian likelihood:\n",
    "\n",
    "+ Take the derivative of $p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n},\\mathbf{w}_{\\mbox{MLE}},\\sigma)$ with respect to $\\sigma$.\n",
    "+ Set to zero, and solve for $\\sigma$.\n",
    "+ You will get:\n",
    "$$\n",
    "\\sigma_{\\mbox{MLE}}^2 = \\frac{\\lVert \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\rVert^2}{n}.\n",
    "$$\n",
    "\n",
    "### Making Predictions\n",
    "How do we make predictions about $y$ at a new point $\\mathbf{x}$?\n",
    "We just use the laws of probability...\n",
    "For the Gaussian likelihood, the **point predictive distribution** is:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{w}_{\\mbox{MLE}}, \\sigma^2_{\\mathbf{\\mbox{MLE}}}) = \n",
    "\\mathcal{N}\\left(y\\middle|\\mathbf{w}_{\\mbox{MLE}}^T\\mathbf{\\phi}(\\mathbf{x}), \\sigma_{\\mbox{MLE}}^2\\right).\n",
    "$$\n",
    "\n",
    "### Examples\n",
    "\n",
    "After watching the video, see [Hands-on Activity 14.1](https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/activities/hands-on-14.1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic regression II (maximum a posteriori estimates)\n",
    "\n",
    "This version of probabilistic is similar to maximum likelihood in the sense that you maximum the log probability of something (the posterior instead of the likelihood) and it has the adendum that it can help you avoid overfitting.\n",
    "\n",
    "Just like before, we wish to model the data using some **fixed** basis/features:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "}\n",
    "$$\n",
    "Again, we *model the measurement process* using a **likelihood** function:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma \\sim N(\\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "}, \\sigma^2).\n",
    "$$\n",
    "The new ingredient is that we *model the uncertainty in the model parameters* using a **prior**:\n",
    "$$\n",
    "\\mathbf{w} \\sim p(\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "### Gaussian Prior on the Weights\n",
    "The Gaussian prior is the simplest possible choice for the weights.\n",
    "It is:\n",
    "$$\n",
    "p(\\mathbf{w}|\\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{0},\\alpha^{-1}\\mathbf{I}\\right) = \n",
    "\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\frac{m}{2}}\\exp\\left\\{-\\frac{\\alpha}{2}\\lVert\\mathbf{w}\\rVert^2\\right\\}.\n",
    "$$\n",
    "The interpretation is that, before we see the data, we beleive that $\\mathbf{w}$ must be around zero with a precision of $\\alpha$.\n",
    "This push to the weights to be towards zero is exactly what helps us avoid overfitting.\n",
    "The bigger the precision parameter $\\alpha$ the more the weights are pushed towards zero.\n",
    "\n",
    "### Graphical representation of the model\n",
    "Let's visualize the regression model as a graph.\n",
    "Remember that the shaded nodes are assumed to be observed (so below we are assuming that we know $\\alpha$ and $\\sigma$).\n",
    "Another thing to observe is that the nodes that are inside the box are repeated as many times as indicated.\n",
    "This is the so-called [plate notation](https://en.wikipedia.org/wiki/Plate_notation) for graphical models and it saves from the troubkle of drawing $n$ input-output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "g = Digraph('bayes_regression')\n",
    "g.node('alpha', label='<&alpha;>', style='filled')\n",
    "g.node('w', label='<<b>w</b>>')\n",
    "g.node('sigma', label='<&sigma;>', style='filled')\n",
    "with g.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('xj', label='<<b>x</b><sub>j</sub>>', style='filled')\n",
    "    sg.node('yj', label='<y<sub>j</sub>>', style='filled')\n",
    "    sg.attr(label='j=1,...,n')\n",
    "    sg.attr(labelloc='b')\n",
    "g.edge('alpha', 'w')\n",
    "g.edge('sigma', 'yj')\n",
    "g.edge('w', 'yj')\n",
    "g.edge('xj', 'yj')\n",
    "g.render('bayes_regression', format='png')\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Posterior of the Weights\n",
    "\n",
    "Combining the likelihood and the prior, we get using Bayes' rule:\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma,\\alpha) = \n",
    "\\frac{p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma)p(\\mathbf{w}|\\alpha)}\n",
    "{\\int p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}', \\sigma)p(\\mathbf{w}'|\\alpha)d\\mathbf{w}'}.\n",
    "$$\n",
    "The posterior summarizes our state of knowledge about $\\mathbf{w}$ after we see the data,\n",
    "if we know $\\alpha$ and $\\sigma$.\n",
    "\n",
    "### Maximum Posterior Estimate\n",
    "We can find a point estimate of $\\mathbf{w}$ by solving:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MPE}} = \\arg\\max_{\\mathbf{w}} p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma)p(\\mathbf{w}|\\alpha).\n",
    "$$\n",
    "For Gaussian likelihood and weight prior, the logarithm of the posterior is:\n",
    "$$\n",
    "\\log p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma,\\alpha) = \n",
    "- \\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2 -\\frac{\\alpha}{2}\\lVert\\mathbf{w}\\rVert^2.\n",
    "$$\n",
    "Taking derivatives with respect to $\\mathbf{w}$ and setting them equal to zero (necessary condition), we find:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MPE}} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "Unfortunately, we no longer have an analytic formula for $\\sigma$... (we will fix that later).\n",
    "\n",
    "### Examples\n",
    "\n",
    "After watching the video, see [Hands-on Activity 14.2](https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/activities/hands-on-14.2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic regression III (Bayesian linear regression)\n",
    "\n",
    "This has the same setup version III of probabilistic regression but we do not just get a point estimate for the weights.\n",
    "We retain the posterior of the weughts in its full complexity.\n",
    "The adendum is that we can now quantify the epistemic uncertainty induced by the limited number of observations used to estimate the weights.\n",
    "\n",
    "For Gaussian likelihood and weight prior, the posterior of the weights is Gaussian:\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma, \\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{m}, \\mathbf{S}\\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{S} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)^{-1},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{m} = \\sigma^{-2}\\mathbf{S}\\Phi^T\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "In the general case of non-Gaussian likelihood (and non-linear models), the posterior will not be analytically available. We will learn how to deal with these cases in Lectures 27 and 28 when we talk about generic ways to characterize posteriors.\n",
    "\n",
    "### Posterior Predictive Distribution\n",
    "Using probability theory, we ask: What do we know about $y$ at a new $\\mathbf{x}$ after seeing the data.\n",
    "To answer this question, we just use the sum rule:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{x}_{1:n}, \\mathbf{y}_{1:n}, \\sigma, \\alpha) = \n",
    "\\int p(y | \\mathbf{x}, \\mathbf{w}, \\sigma) p(\\mathbf{w}|\\mathbf{x}_{1:n}, \\mathbf{y}_{1:n},\\sigma,\\alpha)d\\mathbf{w}.\n",
    "$$\n",
    "For the all-Gaussian case, this is analytically available:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{x}_{1:n}, \\mathbf{y}_{1:n}, \\sigma, \\alpha) = \\mathcal{N}\\left(y|m(\\mathbf{x}), s^2(\\mathbf{x})\\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "m(\\mathbf{x}) = \\mathbf{m}^T\\boldsymbol{\\phi}(\\mathbf{x})\\;\\mbox{and}\\;s(\\mathbf{x}) = \\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{S}\\boldsymbol{\\phi}(\\mathbf{x}) + \\sigma^2.\n",
    "$$\n",
    "\n",
    "Notice that the **predictive uncertainty** is:\n",
    "$$\n",
    "s^2(\\mathbf{x}) = \\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{S}\\boldsymbol{\\phi}(\\mathbf{x}) + \\sigma^2,\n",
    "$$\n",
    "where:\n",
    "+ $\\sigma^2$ corresponds to the measurement noise.\n",
    "+ $\\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{S}\\boldsymbol{\\phi}(\\mathbf{x})$ is the epistemic uncertainty induced by limited data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
