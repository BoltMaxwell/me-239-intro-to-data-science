{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 8 - Analytical examples of Bayesian inference\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop systematic ways for assigning probabilities.\n",
    "+ To introduce Bayesian parameter estimation.\n",
    "\n",
    "## References\n",
    "\n",
    "+ These notes.\n",
    "\n",
    "## How do we come up with the right probability assignments?\n",
    "\n",
    "In applications we often found ourselves in a situation where we have to pick prior probabilities of a given variable. That is, pick probabilities before we see any specific data from that variable.\n",
    "An important question is how we come up with these prior probabilities.\n",
    "Is there a systematic theoretical framework we could follow?\n",
    "There are basically three widely accepted ways:\n",
    "\n",
    "+ The principle of insufficient reason.\n",
    "+ The principle of maximum entropy.\n",
    "+ The principle of transformation groups.\n",
    "\n",
    "In this lecture, we will explain only the first one. The two others, the principle of maximum entropy and the principle of transformation groups, are rather advanced and we will not discuss it.\n",
    "\n",
    "## Principle of Insufficient Reason\n",
    "\n",
    "The principle of insufficient reason has its origins to Laplace. The original statement was:\n",
    "> The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible.\n",
    "*Pierre-Simon Laplace*\n",
    "\n",
    "Let's restate this in simpler terms.\n",
    "Assume that the random variable $X$ can take $N$ possible values, $1, 2,\\dots,N$.\n",
    "If this is all we know about this random variable then *the principle of insufficient reason* tells us to set:\n",
    "$$\n",
    "p(x) = \\frac{1}{N},\n",
    "$$\n",
    "for $x$ in $\\{1,2,\\dots,N\\}$.\n",
    "That is, the principle of insufficient reason tells us to assign the same probability to each possibility.\n",
    "Intuitively, any other choice we could make would introduce a bias towards one value or another.\n",
    "\n",
    "### Example: Throwing a six-sided die\n",
    "Consider a six-sided die with sides numbered $1$ to $6$.\n",
    "Call $X$ the random variable corresponding to an experiment of throwing the die.\n",
    "What is the probability of the die taking a specific value.\n",
    "Using the principle of insufficient reason, we set:\n",
    "$$\n",
    "p(X=x) = \\frac{1}{6}.\n",
    "$$\n",
    "\n",
    "## Bayesian Parameter Estimation\n",
    "\n",
    "Assume that we have a model that predicts the result of a random variable $X$.\n",
    "The model has some parameters $\\theta$ which are to be determined from data which consist of $N$ independent measurements of $X$, i.e., the data are:\n",
    "$$\n",
    "x_{1:N} = (x_1,\\dots,x_N).\n",
    "$$\n",
    "We can write the model as:\n",
    "$$\n",
    "x_n|\\theta \\sim p(x_n|\\theta),\n",
    "$$\n",
    "where we started abusing the mathematical that requires us to use capital letters of random variables and lower case letters for data.\n",
    "But this is just simpler and this is the notation we will be following from now on.\n",
    "The term $p(x_n|\\theta)$ is known as the *likelihood* of this data point.\n",
    "The likelihood of the entire dataset $x_{1:N}$ is just the joint probability density of all observations, i.e., $p(x_{1:N}|\\theta)$.\n",
    "Because all observations are indpendent conditioned on the model parameters $\\theta$, we have:\n",
    "$$\n",
    "p(x_{1:N}|\\theta) = \\prod_{n=1}^Np(x_n|\\theta).\n",
    "$$\n",
    "Be careful. This factorization of the likelihood is not valid if the measurements are not independent.\n",
    "In that case, you need to keep the entire thing together (or factorize it in the way that it factorizes...)\n",
    "\n",
    "To close the loop, we need to say what we think about the pameters before we see any data.\n",
    "In Bayesian jargon, we need to specify our *prior state of knowledge*, or simple our *prior*:\n",
    "$$\n",
    "\\theta \\sim p(\\theta).\n",
    "$$\n",
    "\n",
    "The situation can be discribed graphical as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "gcp = Digraph('coin_toss_bayes_plate')\n",
    "gcp.node('theta', label='<&theta;>')\n",
    "with gcp.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('xn', label='<x<sub>n</sub>>', style='filled')\n",
    "    sg.attr(label='n=1,...,N')\n",
    "    sg.attr(labelloc='b')\n",
    "gcp.edge('theta', 'xn')\n",
    "gcp.render('coin_toss_bayes_plate', format='png')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to apply Bayes' rule to find the posterior.\n",
    "Recall the Bayes' rule for propositions $A$ and $B$:\n",
    "$$\n",
    "p(A|B) = \\frac{p(AB)}{p(B)}.\n",
    "$$\n",
    "We set here\n",
    "$$\n",
    "A = \\text{the model parameters} = \\theta,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "B = \\text{the data} = x_{1:N}.\n",
    "$$\n",
    "That is the question we ask is \"what is the probability of the model parameters given the data?\"\n",
    "Let's just blindly follow the rule:\n",
    "$$\n",
    "p(\\text{the model parameters}|\\text{the data}) = \\frac{p(\\text{the data and the parameters})}{p(\\text{the data})}.\n",
    "$$\n",
    "This $p(\\text{the model parameters}|\\text{the data})$ has a special name.\n",
    "It is our *posterior state of knowledge* about the model parameters or simply the *posterior*.\n",
    "\n",
    "We are ready to proceed.\n",
    "What is the joint probability of the data and the parameters?\n",
    "Well, we can use the Bayes rule again, but in this form:\n",
    "$$\n",
    "p(AB) = p(B|A)p(A).\n",
    "$$\n",
    "In our example:\n",
    "$$\n",
    "p(\\text{the data and the parameters}) = p(\\text{the data} | \\text{the parameters}) p(\\text{the parameters}).\n",
    "$$\n",
    "Putting it all together we get:\n",
    "$$\n",
    "\\text{posterior} = p(\\text{the model parameters}|\\text{the data}) = \n",
    "\\frac{p(\\text{the data} | \\text{the parameters}) p(\\text{the parameters})}{p(\\text{the data})}\n",
    "$$\n",
    "But we have given special names to the terms on the right:\n",
    "$$\n",
    "p(\\text{the data} | \\text{the parameters}) = p(x_{1:N}|\\theta) = \\text{likelihood},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(\\text{the parameters}) = p(\\theta) = \\text{prior}.\n",
    "$$\n",
    "So, we can now write the mnemonic (ignoring the normalization constant):\n",
    "$$\n",
    "\\text{posterior} \\propto \\text{likelihood}\\times\\text{prior}.\n",
    "$$\n",
    "Tracking back our symbols, this can be written mathematically as:\n",
    "$$\n",
    "p(\\theta | x_{1:N}) \\propto p(x_{1:N}|\\theta)p(\\theta).\n",
    "$$\n",
    "*The posterior is everything a Bayesian has to say about the parameter estimation problem.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
