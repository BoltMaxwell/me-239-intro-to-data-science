{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 8.4: Posterior Predictive Checking\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Introduce the concept of posterior predictive checking\n",
    "\n",
    "## Posterior Predictive Checking\n",
    "\n",
    "Assume that we have built a model using some data, say $x_{1:n}$.\n",
    "Now, assume that you do the experiment again under the same conditions.\n",
    "What data does your model tell you that you would observe?\n",
    "The posterior predictive distribution of the *replicated data* $x^{\\text{rep}}_{1:n}$ is simply:\n",
    "$$\n",
    "p(x^{\\text{rep}}_{1:n}|x_{1:n}) = \\int p(x^{\\text{rep}}_{1:n}|\\theta)p(\\theta|x_{1:n})d\\theta,\n",
    "$$\n",
    "where $p(x^{\\text{rep}}_{1:n}|\\theta)$ is just the likelihood and $p(\\theta|x_{1:n})$ the posterior.\n",
    "The idea of *posterior predictive checking* is to repeatedly sample $x^{\\text{rep}}_{1:n}$ and compare their characteristics to the true data.\n",
    "You may reject a model that performs very poorly under predictive checking.\n",
    "However, you cannot accept a model that performs very well.\n",
    "There are other methods for doing this which are more involved.\n",
    "Predictive checking is good for identifying bugs in your code or coming up with ideas to extend the models in a way that better matches the data.\n",
    "\n",
    "## Visual Inspections of Replicated Data\n",
    "\n",
    "The idea here is to simply sample $x^{\\text{rep}}_{1:n}$ and compare it visually to $x_{1:n}$.\n",
    "Let's see this on the coin toss example.\n",
    "\n",
    "## Demonstration with the Coin Toss Example\n",
    "\n",
    "Consider $n$ coin-tosses with probability of heads ($1$) $\\theta$, i.e.,\n",
    "$$\n",
    "x_{i}|\\theta \\sim \\operatorname{Ber}(\\theta).\n",
    "$$\n",
    "We put a uniform prior on $\\theta$:\n",
    "$$\n",
    "\\theta\\sim U([0,1]),\n",
    "$$\n",
    "and we have already seen that the posterior is a Beta:\n",
    "$$\n",
    "\\theta|x_{1:n} \\sim \\operatorname{Beta}\\left(1 + \\sum_{i=1}^nx_i, 1 + n - \\sum_{i=1}^nx_i\\right).\n",
    "$$\n",
    "Now, it is actually possible to get $p(x^{\\text{rep}}_{1:n} | x_{1:n})$ analytically in this case.\n",
    "However, we will not bother doing it.\n",
    "We can simply sampling from it as follows:\n",
    "+ First, sample a $\\theta$ from the posterior $p(\\theta|x_{1:n})$.\n",
    "+ Second, sample a $x^{\\text{rep}}$ from the likelihood $p(x^{\\text{rep}}|\\theta)$.\n",
    "+ Repeat steps one and two as many times as needed.\n",
    "\n",
    "Ok, let's start with a dataset that is 100\\% known that it is compatible from the model because we are simply simulating it.\n",
    "Here you go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# Take a fake coin which is a little bit biased - Presumably unknown to us\n",
    "theta_true = 0.5\n",
    "# Here is the underlying random variable from which we will sample\n",
    "X = st.bernoulli(theta_true)\n",
    "# Sample from it a number of times to generate our y = (y1, ..., xn)\n",
    "n = 50\n",
    "x = X.rvs(size=n)\n",
    "# Now we are ready to calculate the posterior which the Beta we have above\n",
    "alpha = 1.0 + x.sum()\n",
    "beta = 1.0 + n - x.sum()\n",
    "Theta_post = st.beta(alpha, beta)\n",
    "# Now we can plot the posterior PDF for theta\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "thetas = np.linspace(0, 1, 100)\n",
    "ax.plot([theta_true], [0.0], 'o', markeredgewidth=2, markersize=10, label='True value')\n",
    "ax.plot(thetas, Theta_post.pdf(thetas), label=r'$p(\\theta|x_{1:n})$')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_title('$n={0:d}$'.format(n))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, let's draw many replicated data $y^{\\text{rep}}$ from the posterior predictive and compare them to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw replicated data\n",
    "n_rep = 9\n",
    "x_rep = np.ndarray((n_rep, n))\n",
    "for i in range(n_rep):\n",
    "    theta_post_sample = Theta_post.rvs()\n",
    "    x_rep[i, :] = st.bernoulli(theta_post_sample).rvs(size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the data as images.\n",
    "The first row of pixels is are the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 10), dpi=150)\n",
    "ax.imshow(np.vstack([x, x_rep]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visual inspection does not reveal anything strange.\n",
    "Let's now repeat this excersize with a dataset that is completely artificial and does not match the model.\n",
    "This a dataset that we are making by hand trying to emulate a coin toss with probability of heads equal to $0.8$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
    "alpha_2 = 1.0 + x_2.sum()\n",
    "beta_2 = 1.0 + n - x_2.sum()\n",
    "Theta_2_post = st.beta(alpha_2, beta_2)\n",
    "# Now we can plot the posterior PDF for theta\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "thetas = np.linspace(0, 1, 100)\n",
    "ax.plot(thetas, Theta_2_post.pdf(thetas), label=r'$p(\\theta|x_{1:n})$')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_title('$n={0:d}$'.format(n))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no true value for $\\theta$ here as I actually picked the data by hand.\n",
    "Let's see if the visual comparison to replicated data reveals that the data did not really come from a fair coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw replicated data\n",
    "n_rep = 9\n",
    "x_2_rep = np.ndarray((n_rep, n))\n",
    "for i in range(n_rep):\n",
    "    theta_2_post_sample = Theta_2_post.rvs()\n",
    "    x_2_rep[i, :] = st.bernoulli(theta_2_post_sample).rvs(size=n)\n",
    "fig, ax = plt.subplots(figsize=(30, 10), dpi=150)\n",
    "ax.imshow(np.vstack([x_2, x_2_rep]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay close attention you will notice that the data I picked by hand has more transitions from heads to tails than the replicated data.\n",
    "In other words, the replicated data seem to have longer consecutive series of either heads or tails.\n",
    "How can we see this more clearly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quantities and visual inspections\n",
    "\n",
    "We use *test quantities* to characterize the discrepancy between the model and the data.\n",
    "In particular, test quantities help us zoom into the characteristsics of the data that are of particular interest to us.\n",
    "Mathematically, a test quantity is a scalar function of the data and the model parameters $T(x_{1:n},\\theta)$.\n",
    "There are some general recipies for creating test quantities for regression and classification.\n",
    "However, in general, you must use common sense in selecting them.\n",
    "What are the important characteristics of the data that your model should be capturing.\n",
    "We will be seeing specific examples below.\n",
    "\n",
    "Now, assume that you have selected one, or more, test quantities.\n",
    "What do you do with them?\n",
    "Well, the easiest thing to do is a visual comparison of the histogram of the test quantity over replicated data, i.e., of\n",
    "$p(T(x^{\\text{rep}}_{1:n},\\theta) | x_{1:n})$, and compare it to the observe value $T(x_{1:n},\\theta)$.\n",
    "In these plots you are trying to see how likely or unlikely is observed test quantity according to your model.\n",
    "We will be visualizing them next for the coin toss example.\n",
    "\n",
    "### Test quantities and Bayesian $p$-values\n",
    "\n",
    "Antother thing that you can do to check your model is to evaluate the probability that the replicated data give you a test quantity that is more extreme than the observed value.\n",
    "This probability is known as the posterior (or Bayesian) $p$-value and it is defined by:\n",
    "$$\n",
    "p_B = \\mathbb{P}(T(x^{\\text{rep}}_{1:n},\\theta) > T(x_{1:n},\\theta) | x_{1:n}) = \\int 1_{[T(x_{1:n},\\theta),\\infty]}(T(x^{\\text{rep}}_{1:n})) p(x^{\\text{rep}}_{1:n}, \\theta|x_{1:n}) dx^{\\text{rep}}_{1:n}d\\theta.\n",
    "$$\n",
    "Of course, you can just estimate the Bayesian $p$-value using Monte Carlo sampling from the joint posterior of $x^{\\text{rep}}_{1:n}$ and $\\theta$ conditioned on the data $x_{1:n}$.\n",
    "\n",
    "How should I interpret the Bayesian $p$-values?\n",
    "**It is not the probability that your model is correct.**\n",
    "We will derive this probability in the Bayesian model selection lecture.\n",
    "The Bayesian $p$-value is the probability that replications of the experiment will yield test quantitys that exceed the observed value under the assumption that your model is correct.\n",
    "So, here is a nice way to interpret them:\n",
    "+ A Bayesian $p$-value close to $0$ or $1$ indicates that the observed test quantity is unlikely under the assumption that your model is correct. So your model does not capture this aspect of the data. You probably need expand/modify your model somehow.\n",
    "\n",
    "## Back to the coin toss example\n",
    "\n",
    "What are some good test quantities that we can pick for this example.\n",
    "An obvious one is the number of heads.\n",
    "This is only a function of the data.\n",
    "It is:\n",
    "$$\n",
    "T_{h}(x_{1:n}) = \\sum_{i=1}^nx_i.\n",
    "$$\n",
    "Let's implement this as a Python function of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_h(x):\n",
    "    \"\"\"\n",
    "    This is an implementation of a \n",
    "    \"\"\"\n",
    "    return x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that we have run this example with two datasets: one that was generated from the correct model and one that was generated by hand.\n",
    "We will see the results that we get from both.\n",
    "For the first dataset (the one generated by the model) we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observed test quantity\n",
    "T_h_obs = T_h(x)\n",
    "print('The observed test quantity is {0:d}'.format(T_h_obs))\n",
    "# Draw replicated data\n",
    "n_rep = 1000\n",
    "x_rep = np.ndarray((n_rep, n))\n",
    "for i in range(n_rep):\n",
    "    theta_post_sample = Theta_post.rvs()\n",
    "    x_rep[i, :] = st.bernoulli(theta_post_sample).rvs(size=n)\n",
    "# Evaluate the test quantity\n",
    "T_h_rep = np.ndarray(x_rep.shape[0])\n",
    "for i in range(x_rep.shape[0]):\n",
    "    T_h_rep[i] = T_h(x_rep[i, :])\n",
    "# Estimate the Bayesian p-value\n",
    "p_val = np.sum(np.ones((n_rep,))[T_h_rep > T_h_obs]) / n_rep\n",
    "print('The Bayesian p_value is {0:1.4f}'.format(p_val))\n",
    "# Do the plot\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "tmp = ax.hist(T_h_rep, density=True, alpha=0.25, label='Replicated test quantity')[0]\n",
    "ax.plot(T_h_obs * np.ones((50,)), np.linspace(0, tmp.max(), 50), 'k', label='Observed test quantity')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look also at the other dataset (the one we picked by hand):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observed test quantity\n",
    "T_h_2_obs = T_h(x_2)\n",
    "print('The observed test quantity is {0:d}'.format(T_h_2_obs))\n",
    "# Draw replicated data\n",
    "n_rep = 1000\n",
    "x_2_rep = np.ndarray((n_rep, n))\n",
    "for i in range(n_rep):\n",
    "    theta_2_post_sample = Theta_2_post.rvs()\n",
    "    x_2_rep[i, :] = st.bernoulli(theta_2_post_sample).rvs(size=n)\n",
    "# Evaluate the test quantity\n",
    "T_h_2_rep = np.ndarray(x_2_rep.shape[0])\n",
    "for i in range(x_rep.shape[0]):\n",
    "    T_h_2_rep[i] = T_h(x_2_rep[i, :])\n",
    "# Estimate the Bayesian p-value\n",
    "p_val_2 = np.sum(np.ones((n_rep,))[T_h_2_rep > T_h_2_obs]) / n_rep\n",
    "print('The Bayesian p_value is {0:1.4f}'.format(p_val_2))\n",
    "# Do the plot\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "tmp = ax.hist(T_h_2_rep, density=True, alpha=0.25, label='Replicated test quantity')[0]\n",
    "ax.plot(T_h_2_obs * np.ones((50,)), np.linspace(0, tmp.max(), 50), 'k', label='Observed test quantity')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks about the same. This just means that I was able to replicate this particular test quantity when I picked values by hand.\n",
    "Can we find a better statistic?\n",
    "Remember our observation when we plotted the replicated data vs the true data for the second case.\n",
    "We observed that my hand-picked data included more transitions from heads to tails.\n",
    "Let's build a statistic that captures that.\n",
    "We are going to take this:\n",
    "$$\n",
    "T_s(x) = \\text{# number of switches from 0 and 1 in the sequence}\\;x.\n",
    "$$\n",
    "This is not easy to write in an analytical form, but we can program it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_s(x):\n",
    "    s = 0\n",
    "    for i in range(1, x.shape[0]):\n",
    "        if x[i] != x[i-1]:\n",
    "            s += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look first at the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observed test quantity\n",
    "T_s_obs = T_s(x)\n",
    "print('The observed test quantity is {0:d}'.format(T_s_obs))\n",
    "# Draw replicated data\n",
    "n_rep = 1000\n",
    "x_rep = np.ndarray((n_rep, n))\n",
    "for i in range(n_rep):\n",
    "    theta_post_sample = Theta_post.rvs()\n",
    "    x_rep[i, :] = st.bernoulli(theta_post_sample).rvs(size=n)\n",
    "# Evaluate the test quantity\n",
    "T_s_rep = np.ndarray(x_rep.shape[0])\n",
    "for i in range(x_rep.shape[0]):\n",
    "    T_s_rep[i] = T_s(x_rep[i, :])\n",
    "# Estimate the Bayesian p-value\n",
    "p_val = np.sum(np.ones((n_rep,))[T_s_rep > T_s_obs]) / n_rep\n",
    "print('The Bayesian p_value is {0:1.4f}'.format(p_val))\n",
    "# Do the plot\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "tmp = ax.hist(T_h_rep, density=True, alpha=0.25, label='Replicated statistic')[0]\n",
    "ax.plot(T_s_obs * np.ones((50,)), np.linspace(0, tmp.max(), 50), 'k', label='Observed test quantity')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks okay. \n",
    "\n",
    "Let's now look at the one I picked by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observed test quantity\n",
    "T_s_2_obs = T_s(x_2)\n",
    "print('The observed test quantity is {0:d}'.format(T_s_2_obs))\n",
    "# Draw replicated data\n",
    "n_rep = 1000\n",
    "x_2_rep = np.ndarray((n_rep, n))\n",
    "for i in range(n_rep):\n",
    "    theta_2_post_sample = Theta_2_post.rvs()\n",
    "    x_2_rep[i, :] = st.bernoulli(theta_2_post_sample).rvs(size=n)\n",
    "# Evaluate the test quantity\n",
    "T_s_2_rep = np.ndarray(x_2_rep.shape[0])\n",
    "for i in range(x_rep.shape[0]):\n",
    "    T_s_2_rep[i] = T_s(x_2_rep[i, :])\n",
    "# Estimate the Bayesian p-value\n",
    "p_val_2 = np.sum(np.ones((n_rep,))[T_s_2_rep > T_s_2_obs]) / n_rep\n",
    "print('The Bayesian p_value is {0:1.4f}'.format(p_val_2))\n",
    "# Do the plot\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "tmp = ax.hist(T_h_2_rep, density=True, alpha=0.25, label='Replicated statistic')[0]\n",
    "ax.plot(T_s_2_obs * np.ones((50,)), np.linspace(0, tmp.max(), 50), 'k', label='Observed test quantity')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are highly unlikely under the assumptions of this model.\n",
    "\n",
    "## Questions\n",
    "+ Rerun all the steps above with a larger number $N$ of coin toss observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
