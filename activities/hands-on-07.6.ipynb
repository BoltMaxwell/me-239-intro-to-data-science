{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 7.6: Propagating Uncertainties through an Initial Value Problem\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- To propagate uncertainties through an initial value problem.\n",
    "\n",
    "### Uncertainty propagation through an ordinary differential equation\n",
    "\n",
    "Ordinary differential equations (ODEs) are commonly used to model the response of physical systems.\n",
    "Very often, one may be uncertain about the parameters of these ODEs, e.g., physical constants, or even their initial conditions.\n",
    "The question is how does this uncertainty affect the quantity predicted by the ODE?\n",
    "How can you mathematically describe this effect, how can you estimate it, and how can you visualize it?\n",
    "We are going to address these three questions using the a very simple example.\n",
    "\n",
    "### Exponential decay differential equation and its solution\n",
    "Consider the prototypical ODE describing the exponential decay of a quantity $y$ over time:\n",
    "$$\n",
    "    \\dot{y} = \\frac{d y}{dt} =-ay(t).\n",
    "$$\n",
    "The equation says that the quantity $y$ is decreasing at a rate proportional to itself.\n",
    "The rate $a$ is the positive variable known as the *exponential decay constant* or *exponential decay rate*.\n",
    "The exponential decay rate has units of inverse time and it corresponds to the rate with which a unit of $y$ is lost per unit of time. The larger the exponential decay, the faster $y$ dies out over time.\n",
    "This equation has a plethora of possible interpretations ranging from chemical reactions to electrostatics to heat transfer to radioactivity to thermoelectricity to vibrations.\n",
    "\n",
    "To solve the equation, we also need *initial conditions*, basically how much $y$ we have at time $t=0$, i.e.,\n",
    "$$\n",
    "y(0) = y_0.\n",
    "$$\n",
    "The ODE with the initial conditions is called an *initial value problem* (IVP).\n",
    "For this particular equation, the solution of the initial value problem is analytically available:\n",
    "$$\n",
    "y(t) = y_0e^{-at}.\n",
    "$$\n",
    "\n",
    "### Assigning random variables to uncertaint quantities\n",
    "\n",
    "Let's assume that we are uncertain about the decay rate coefficient and the initial conditions.\n",
    "To model this, we are going to turn both these parameters into random variables.\n",
    "To do this, we are going to write down everything we know about these parameters and then pick compatible probability distributions.\n",
    "\n",
    "We start with the decay rate.\n",
    "We know that it has to be positive.\n",
    "We have to assign to $a$ a probability distribution with positive support, i.e., a probability distribution that puts zero probability on values that are not positive.\n",
    "Two of the most commonly used probability distributions are the [Exponential](https://en.wikipedia.org/wiki/Exponential_distribution) and the [Log-Normal](https://en.wikipedia.org/wiki/Log-normal_distribution).\n",
    "Which one should we pick?\n",
    "It depends on what we know about it.\n",
    "Let's say that we expect it to be around $0.1$ (units of inverse time) and that's all we know.\n",
    "Mathematically, this means that we want the expectation of $a$ to be:\n",
    "$$\n",
    "\\mathbb{E}[a] = 0.1\n",
    "$$\n",
    "If that's all you know then the legitimate thing to do is to assign to $a$ an Exponential distribution with a rate parameter that gives you the right expectation.\n",
    "Why is this the legitimate thing to do?\n",
    "Well, that's a big topic which we are going to discuss at a later lecture when we talk about the [principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy).\n",
    "The short answer is that *the Exponential is the least biased distribution with positive support and a known expectation*.\n",
    "Mathematically, we write:\n",
    "$$\n",
    "a\\sim \\operatorname{Exp}(\\lambda),\n",
    "$$\n",
    "and we need to pick $\\lambda$ so that $\\mathbb{E}[a] = \\lambda^{-1} = 0.1$.\n",
    "So, we have to pick $\\lambda = 10$.\n",
    "Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to define random variables and sample from them\n",
    "import scipy.stats as st\n",
    "\n",
    "# The random variable corresponding to the decay rate a:\n",
    "# (Please note that the implementation of the exponential in scipy.stats requires the inverse of lambda\n",
    "# not lambda, read https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html:\n",
    "#          A common parameterization for expon is in terms of the rate parameter lambda, \n",
    "#          such that pdf = lambda * exp(-lambda * x). \n",
    "#          This parameterization corresponds to using scale = 1 / lambda.\n",
    "# ALWAYS READ THE DOCS!!!\n",
    "# )\n",
    "a = st.expon(scale=1.0 / 10.)\n",
    "\n",
    "# Ploting the PDF of a\n",
    "a_vals = np.linspace(0.0, 1, 100)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(a_vals, a.pdf(a_vals))\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$p(\\alpha)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have selected a probability model for the rate parameter $a$.\n",
    "Let's move to the initial conditions $y_0$, which is also positive.\n",
    "This time, let's assume that we have an expectation for $y_0$, say that we believe that:\n",
    "$$\n",
    "\\mathbb{E}[y_0] = 10.\n",
    "$$\n",
    "If that's all we have, then we would do exactly what we did for the decay rate and proceed by assigning an Exponential distribution to $y_0$ with a properly chosen rate parameter.\n",
    "But, let's assume we know more. Let's say that we have some idea about the variance of $y_0$.\n",
    "For example, we may believe that:\n",
    "$$\n",
    "\\mathbb{V}[y_0] = 1.\n",
    "$$\n",
    "Now, the legitimate thing to do is to choose a Log-Normal distribution with the right $\\mu$ and $\\sigma$ parameters, i.e.,\n",
    "$$\n",
    "y_0 \\sim \\operatorname{Lognormal}(\\mu,\\sigma^2)\n",
    "$$\n",
    "Again, this is based on the [principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) and it can be interpreted as the *least biased distribution with positive support with a known expectation and variance*.\n",
    "Now we can match $\\mu$ and $\\sigma$ to the available information.\n",
    "For the expectation, we have:\n",
    "$$\n",
    "\\mathbb{E}[y_0] = \\exp\\left\\{\\mu + \\frac{1}{2}\\sigma\\right\\} = 10.\n",
    "$$\n",
    "For the variance, we have:\n",
    "$$\n",
    "\\mathbb{V}[y_0] = \\left[\\exp\\{\\sigma^2\\} - 1\\right]\\exp\\left\\{2\\mu + \\sigma^2\\right\\} = 1.\n",
    "$$\n",
    "We need to solve this system of equations for $\\mu$ and $\\sigma^2$.\n",
    "Unfortunately, we do not have the time to work this out analytically.\n",
    "Let's solve it numerically using [scipy.optimize.root](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html#scipy.optimize.root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a function of ``mu`` and ``sigma`` \n",
    "# the zero of which corresponds to the solution of the system\n",
    "# above\n",
    "def f(x):\n",
    "    mu = x[0]\n",
    "    sigma = x[1]\n",
    "    res = np.ndarray((2,))\n",
    "    # The first equation\n",
    "    res[0] = np.exp(mu + 0.5 * sigma) - 10.0\n",
    "    # The second equation\n",
    "    res[1] = (np.exp(sigma ** 2) - 1.0) * np.exp(2.0 * mu + sigma ** 2) - 1.0\n",
    "    return res\n",
    "\n",
    "# Now you will also see a standard way to solve a root finding problem\n",
    "from scipy.optimize import root\n",
    "\n",
    "# An initial guess for mu and sigma\n",
    "x0 = [np.log(10.), 1.0]\n",
    "sol = root(f, x0)\n",
    "\n",
    "# Print the result of the optimization\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to ``sol.fun``. Notice that it is very close to zero. This means that the optimization worked.\n",
    "The values we found are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = sol.x[0]\n",
    "sigma = sol.x[1]\n",
    "print('mu = {0:1.2f}'.format(mu))\n",
    "print('sigma = {0:1.2f}'.format(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the corresponding random variable using [scipy.stats.lognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html) and do a sanity check.\n",
    "Please notice that scipy.stats is not using the standard parameteterization of the Lognormal.\n",
    "We have to be a little bit careful with it.\n",
    "**Always read the documentation of the code you are using and make sure you understand what it does.**\n",
    "In particular, pay attention to the following text:\n",
    "\n",
    "> A common parametrization for a lognormal random variable Y is in terms of the mean, ``mu``, and standard deviation, ``sigma``, of the unique normally distributed random variable X such that ``exp(X) = Y``. This parametrization corresponds to setting ``s = sigma`` and ``scale = exp(mu)``.\n",
    "\n",
    "Now, we know how to match things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = st.lognorm(s=sigma, scale=np.exp(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the PDF of this random variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the PDF of y0\n",
    "y0_vals = np.linspace(0.0, 20, 100)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(y0_vals, y0.pdf(y0_vals))\n",
    "ax.set_xlabel(r'$y_0$')\n",
    "ax.set_ylabel(r'$p(y_0)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating uncertainties through the initial value problem\n",
    "\n",
    "As mentioned earlier the solution of the initial value problem at time $t$ is given by the formula:\n",
    "$$\n",
    "y(t) = y_0 e^{-at}.\n",
    "$$\n",
    "Since $y_0$ and $a$ are random variables, the quantities $y(t)$ at any time $t$ are random variables.\n",
    "Whenever you have a random variable parameterized by a label like $t$ (time) you say that you have a *random process*.\n",
    "The first thing we are going to do, is take a few samples of this random process at discrete timesteps.\n",
    "\n",
    "Specifically, consider some $K$ time steps:\n",
    "$$\n",
    "0 = t_1 < t_2 < \\dots t_K = 100\n",
    "$$\n",
    "and the corresponding values of $y$ at these timesteps:\n",
    "$$\n",
    "y_k = y(t_k).\n",
    "$$\n",
    "Then, all the $y_k$'s together form a $K$-dimensional random vector which we are going to sample and analyze.\n",
    "First, here is how you sample it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of timesteps you want to use\n",
    "K = 100\n",
    "# The timesteps\n",
    "ts = np.linspace(0, 100.0, K)\n",
    "\n",
    "# The number of samples to take\n",
    "num_samples = 10000\n",
    "\n",
    "# Sample some a's\n",
    "a_samples = a.rvs(num_samples)\n",
    "\n",
    "# Sample some y0's\n",
    "y0_samples = y0.rvs(num_samples)\n",
    "\n",
    "# For each one of these samples, evaluate the solution of the\n",
    "# initial value problem at all ``K`` timesteps.\n",
    "# We are going to put the results in a ``num_samples x K`` array:\n",
    "y_samples = np.ndarray((num_samples, K))\n",
    "# Loop over all samples\n",
    "for i in range(num_samples):\n",
    "    y_samples[i, :] = y0_samples[i] * np.exp(-a_samples[i] * ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's just plot some of the samples as functions of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(ts, y_samples[:10, :].T, color='r', lw=0.5)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's focus at an intermediate timestep and let's do our usual analysis (pdf and predictive quantiles).\n",
    "Let's just pick something close to $t=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_close_to_one_samples = y_samples[:, 1] # This is now a 1D array, each element being\n",
    "                                  # a sample for ``y(1)``\n",
    "# Find the quantiles\n",
    "y_close_to_one_50 = np.quantile(y_close_to_one_samples, 0.5)\n",
    "y_close_to_one_025 = np.quantile(y_close_to_one_samples, 0.025)\n",
    "y_close_to_one_975 = np.quantile(y_close_to_one_samples, 0.975)\n",
    "print('median y(1) = {0:1.2f}'.format(y_close_to_one_50))\n",
    "print('y(1) is in [{0:1.2f}, {1:1.2f}] with 95% probability'.format(y_close_to_one_025, y_close_to_one_975))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the empirical CDF of y(1)\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "ecdf_y_close_to_one = ECDF(y_close_to_one_samples)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "yys = np.linspace(y_close_to_one_samples.min(), y_close_to_one_samples.max(), 100)\n",
    "ax.plot(yys, ecdf_y_close_to_one(yys))\n",
    "ax.set_xlabel(r'$\\tilde{y}$')\n",
    "ax.set_ylabel(r'$p(y(1) \\leq \\tilde{y})$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the CDF to find the CDF to find the probability that $y(1)$ falls within a given interval.\n",
    "For example, here is the probability that $y(1)$ exceeds $8$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p(y(1) >= 8) = {0:1.2f}'.format(1.0 - ecdf_y_close_to_one(8.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the histogram of $y(1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the histogram of y(1)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.hist(y_close_to_one_samples, density=True, alpha=0.25, bins=100)\n",
    "ax.plot([y_close_to_one_50], [0], 'ro', markersize=5, label='50% quantile (median)')\n",
    "ax.plot([y_close_to_one_025], [0], 'bo', markersize=5, label='2.5% quantile')\n",
    "ax.plot([y_close_to_one_975], [0], 'go', markersize=5, label='97.5% quantile')\n",
    "ax.set_xlabel('$y(1)$')\n",
    "ax.set_ylabel('$p(y(1))$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said in the previous handout, the box plot is a nice way to summarize the predictive quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.boxplot(y_close_to_one_samples, whis=[2.5, 97.5], labels=['y(1)']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all good for one timestep. But how can we analyze multiple timesteps.\n",
    "To begin with, let's subsample our data by taking 10 timesteps between 0 and 2 and by plotting the box plots for each one the corresponding $y(t)$'s.\n",
    "Here is how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_samples_few_timesteps = y_samples[:, ::20] # This all random observations (first :),\n",
    "                                             # but at every 20 timesteps.\n",
    "# The corresponding timesteps are:\n",
    "ts_few = ts[::20]\n",
    "\n",
    "# Let's look at the dimensions of this (number of samples x time of timesteps)\n",
    "print(y_samples_few_timesteps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the box plot for all these timesteps\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.boxplot(y_samples_few_timesteps, whis=[2.5, 97.5], \n",
    "           labels=['$t={0:1.2f}$'.format(t) for t in ts_few]);\n",
    "ax.set_ylabel('$y(t)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because it shows us how the uncertainty evolves over time.\n",
    "Could we do more steps in between? Yes, but the plot would look very messy if we used boxplots.\n",
    "We can manually calculate median and the 95\\% predictive intervals and plot them as functions of time.\n",
    "That is, we are now going to estimate the median $\\mu_{50}(t_k)$ and the quantiles $\\mu_{2.5}(t_k)$ and $\\mu_{97.5}(t_k)$ at all timesteps $t_k$.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can actually do this in one line:\n",
    "mu_50, mu_025, mu_975 = np.quantile(y_samples, # The data (number_of_samples x K)\n",
    "                                    [0.5, 0.025, 0.975], # Which quantiles to take\n",
    "                                    axis=0    # Take quantiles along the first dimension (samples)\n",
    "                                   )\n",
    "# Let's plot these:\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(ts, mu_50, label='Median')\n",
    "ax.plot(ts, mu_025, '--', label='2.5% quantile')\n",
    "ax.plot(ts, mu_975, ':', label='97.5% quantile')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could potentially include more quantiles in-between if you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_to_take = np.linspace(0.025, 0.975, 10)\n",
    "quants = np.quantile(y_samples, # The data (number_of_samples x K)\n",
    "                     quantiles_to_take, # Which quantiles to take\n",
    "                     axis=0    # Take quantiles along the first dimension (samples)\n",
    "                    )\n",
    "# Let's plot these:\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(ts, quants.T, lw=0.5)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(['{0:1.1f}% quantile'.format(q * 100) for q in quantiles_to_take], loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very common way to summarize the uncertainty is to plot the median and then shade the area between the extreme quantiles.\n",
    "Here is how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(ts, mu_50, label='Median')\n",
    "ax.fill_between(ts, mu_025, mu_975, alpha=0.25, label='95% predictive interval')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way you should interpret the predictive interval is that there is a 95% probability that a random trajectory will fall inside (strictly speaking they are *approximately* that).\n",
    "Let's plot 100 random samples along with the shaded area above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(ts, mu_50, label='Median')\n",
    "ax.fill_between(ts, mu_025, mu_975, alpha=0.25, label='95% predictive interval')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(loc='best')\n",
    "ax.plot(ts, y_samples[:100, :].T, 'r', lw=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two other very common things to plot is the expectation, $\\mathbb{E}[y(t)]$, and the variance $\\mathbb{V}[y(t)}$ as functions of time.\n",
    "Let's do them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_y = np.mean(y_samples, axis=0)\n",
    "var_y = np.var(y_samples, axis=0)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(ts, exp_y, label=r'$\\mathbb{E}[y(t)]$')\n",
    "ax.plot(ts, var_y, '--', label=r'$\\mathbb{V}[y(t)]$')\n",
    "ax.set_xlabel('$t$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Notice that the expectation $\\mathbb{E}[y(t)]$ decays with time despite the uncertainty? Why?\n",
    "+ Notice that the variance $\\mathbb{V}[y(t)]$ initially increases, but after a point it starts decreasing.\n",
    "Will the variance continue to decrease beyond the timestep shown in the figure?\n",
    "If yes, this is a rare example of a dynamical system that becomes more predictable as time passes.\n",
    "+ Repeat the analysis above by assuming that you do not know the variance of the initial condition $y_0$.\n",
    "This can be accomplished by replacing the distribution assigned to $y_0$ with an Exponential distribution.\n",
    "Do you have less or more uncertainty now?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
