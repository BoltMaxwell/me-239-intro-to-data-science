{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 9.3: The Generalized Linear Model\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To regression using a generalized linear model\n",
    "\n",
    "## The generalized linear model\n",
    "\n",
    "The form of the generalized linear model is:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})}\n",
    "$$\n",
    "where $\\mathbf{w} = (w_{1}, ... , w_{m})^{T}$ and $\\boldsymbol{\\phi} = (\\phi_{1}, ..., \\phi_{m})^{T}$ are arbitrary basis functions.\n",
    "Note that the model is linear in $\\mathbf{w}$ not $\\mathbf{x}$, but the basis functions $\\boldsymbol{\\phi}(\\mathbf{x})$ can be non-linear.\n",
    "\n",
    "## The polynomial model as a generalized linear model\n",
    "We have already seen an example of a generalized linear model when $\\mathbf{x}$ has only one dimension: the polynomial model.\n",
    "In the polynomial model, the basis functions are:\n",
    "$$\n",
    "\\phi_1(x) = 1,\n",
    "$$\n",
    "$$\n",
    "\\phi_2(x) = x,\n",
    "$$\n",
    "$$\n",
    "\\phi_3(x) = x^2,\n",
    "$$\n",
    "and so on.\n",
    "This is just one of the possible choices.\n",
    "\n",
    "## Multivariate linear regression as a generalized linear model\n",
    "In multivariate linear regression the inputs $\\mathbf{x}$ have $d$ dimensions, say $\\mathbf{x}=(x_1,\\dots,x_d)$.\n",
    "The linear model is:\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_2 + \\dots w_dx_d.\n",
    "$$\n",
    "This is also a generalized linear model with $m=d+1$ basis functions:\n",
    "$$\n",
    "\\phi_1(\\mathbf{x}) = 1,\n",
    "$$\n",
    "$$\n",
    "\\phi_2(\\mathbf{x}) = x_1,\n",
    "$$\n",
    "$$\n",
    "\\phi_3(\\mathbf{x}) = x_2,\n",
    "$$\n",
    "and so on.\n",
    "\n",
    "## Other generalized linear models\n",
    "Some common examples of generalized linear moedls include:\n",
    "+ Multi-dimensional polynomials, $\\phi_j(\\mathbf{x}) = \\sum_{\\alpha\\in\\mathcal{A}_j}\\beta_{\\alpha}\\mathbf{x}^{\\alpha}$,\n",
    "where we are using the [multi-index notation](https://en.wikipedia.org/wiki/Multi-index_notation) to save some space.\n",
    "+ Radial basis functions, $\\phi_j(\\mathbf{x}) = \\exp\\left\\{-\\frac{\\parallel \\mathbf{x} - \\mathbf{x}_j\\parallel^2}{2\\ell^2}\\right\\}$.\n",
    "+ Fourier series, $\\phi_{2j}(x) = \\cos\\left(\\frac{2j\\pi}{L}x\\right)$ and $\\phi_{2j+1}(x)=\\sin\\left(\\frac{2j\\pi}{L}x\\right)$.\n",
    "+ $\\dots$\n",
    "We will play with that last two in this hands-on activity.\n",
    "\n",
    "## Fitting the generalized linear model using least squares\n",
    "\n",
    "The idea is to find the best $\\mathbf{w}$ by minimizing a quadratic loss function:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w})\\equiv\\mathcal{L}(\\mathbf{w};\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}) = \\sum_{i=1}^n\\left[y(\\mathbf{x}_i;\\mathbf{w}) - y_i\\right]^2.\n",
    "$$\n",
    "As we discussed in the lecture, the loss function can be re-expressed as:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathcal{L}(\\mathbf{w}) &=& \\lVert\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\rVert^2\\\\\n",
    "&=& \\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\right)^T\\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\right).\n",
    "\\end{array}\n",
    "$$\n",
    "Here $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{n\\times m}$ is the **design matrix**:\n",
    "$$\n",
    "\\Phi_{ij} = \\phi_j(\\mathbf{x}_j).\n",
    "$$\n",
    "\n",
    "To minimize the loss function, we follow these steps:\n",
    "+ Take the derivative of $\\mathcal{L}(\\mathbf{w})$ with respect to $\\mathbf{w}$.\n",
    "+ Set it equal to zero and solve for $\\mathbf{w}$.\n",
    "+ You will get [(Bishop, 2006)](http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738):\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{LS}} = \\left(\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^{T}\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "\n",
    "To solve this problem, just use:\n",
    "\n",
    "> [numpy.linalg.lstsq](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.lstsq.html)\n",
    "\n",
    "You give it $\\mathbf{\\Phi}$ and $\\mathbf{y}_{1:n}$ and it returns $\\mathbf{w}_{\\mbox{LS}}$.\n",
    "\n",
    "## Example - Motorcycle data with polynomials\n",
    "\n",
    "Let's load the the motorcycle data to demonstrate generalized linear models.\n",
    "Just like before, you need to make sure that the data file is in the current working directory of this Jupyter notebook.\n",
    "The data file is [here](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/motor.dat).\n",
    "You can follow the instructions of [Problem 0 of Homework 3](https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/homework/homework_03.ipynb) to put it in your Google Drive, mount the Google Drive, and then change into that directory.\n",
    "In this handout, we will do something faster: we will just download the data file to the current working directory.\n",
    "This is not a good choice for big files, but it will work just fine for this one.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The url of the file we want to download\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/motor.dat'\n",
    "# If you are interested in understanding what the function download does, see the header\n",
    "# of this Jupyter notebook\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have the file. Let's load it and visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('motor.dat')\n",
    "X = data[:, 0][:, None]\n",
    "Y = data[:, 1]\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with polynomial regression.\n",
    "We just need to write code that calculates the design matrix.\n",
    "Here is tghe code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the design matrix for degree 3 polynomial looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = get_polynomial_design_matrix(X, 3)\n",
    "print(Phi[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the polynomials as a function of $x$ so that you get some intuition about how $y$ is expanded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "xx = np.linspace(0, 60, 200)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], 3)\n",
    "plt.plot(xx, Phi_xx)\n",
    "plt.ylabel(r'$\\phi_i(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now solve the least squares problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the polynomial degree\n",
    "degree = 3\n",
    "# Make the design matrix\n",
    "Phi = get_polynomial_design_matrix(X, degree)\n",
    "# Solve least squares problem\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "# Make prediction at a dense set of points\n",
    "xx = np.linspace(0, 60, 200)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "Y_p = np.dot(Phi_xx, w_LS)\n",
    "# Plot the predictions\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(xx, Y_p, label='LS Prediction (Polynomial Basis)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Experiment with polynomials of degree 4, 5, 10, 20\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which degree (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Motorcycle data with Fourier basis\n",
    "\n",
    "Let's now repeat what we did with polynomial regression with a Fourier basis.\n",
    "The mathematical form of the basis is:\n",
    "$$\n",
    "\\phi_{2j}(x) = \\cos\\left(\\frac{2j\\pi}{L}x)\\right),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\phi_{2j+1}(x) = \\sin\\left(\\frac{2j\\pi}{L}x)\\right),\n",
    "$$\n",
    "for $j=1,\\dots,m/2$.\n",
    "First, we write code that computes the design matrix for the new basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier_design_matrix(x, L, num_terms):\n",
    "    \"\"\"\n",
    "    Fourier expansion with ``num_terms`` cosines and sines.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        L           -       The \"length\" of the domain.\n",
    "        num_terms   -       How many Fourier terms do you want. This is not the number\n",
    "                            of basis functions you get. The number of basis functions\n",
    "                            is 1 + num_terms / 2. The first one is a constant.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(int(num_terms / 2)):\n",
    "        cols.append(np.cos(2 * (i+1) * np.pi / L * x))\n",
    "        cols.append(np.sin(2 * (i+1) * np.pi / L * x))\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by visualizing the Fourier basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "xx = np.linspace(0, 60, 200)\n",
    "Phi_xx = get_fourier_design_matrix(xx[:, None], 60.0, 4)\n",
    "plt.plot(xx, Phi_xx)\n",
    "plt.ylabel(r'$\\phi_i(x)$')\n",
    "plt.xlabel('$x$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now solve the least squares problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the parameters of the Fourier basis\n",
    "L = 60.0\n",
    "num_terms = 4\n",
    "# Make the design matrix\n",
    "Phi = get_fourier_design_matrix(X, L, num_terms)\n",
    "# Solve least squares problem\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "# Make prediction at a dense set of points\n",
    "xx = np.linspace(0, 60, 200)\n",
    "Phi_xx = get_fourier_design_matrix(xx[:, None], L, num_terms)\n",
    "Y_p = np.dot(Phi_xx, w_LS)\n",
    "# Plot the predictions\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(xx, Y_p, label='LS Prediction (Fourier Basis)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Experiment with 4, 10, 20, 40, terms.\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which one (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Motorcycle data with Fourier basis\n",
    "\n",
    "Let's now try out the radial basis functions.\n",
    "The mathematical form is:\n",
    "$$\n",
    "\\phi_i(x) = \\exp\\left\\{-\\frac{(x-x_i^c)^2}{2\\ell^2}\\right\\},\n",
    "$$\n",
    "where $x_i^c$ are points about each the basis functions are centered.\n",
    "We start with the code that evaluates the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbf_design_matrix(x, x_centers, ell):\n",
    "    \"\"\"\n",
    "    Radial basis functions design matrix.\n",
    "    \n",
    "    Arguments:\n",
    "        x          -     the input points on which you want to evaluate the\n",
    "                         design matrix\n",
    "        x_center   -     the centers of the radial basis functions\n",
    "        ell        -     the lengthscale of the radial basis function\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(x_centers.shape[0]):\n",
    "        cols.append(np.exp(-(x - x_centers[i]) ** 2 / ell))\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "xx = np.linspace(0, 60, 200)\n",
    "ell = 5.\n",
    "num_terms = 10\n",
    "x_centers = np.linspace(0, 60, num_terms)\n",
    "Phi_xx = get_rbf_design_matrix(xx[:, None], x_centers, ell)\n",
    "plt.plot(xx, Phi_xx)\n",
    "plt.ylabel(r'$\\phi_i(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's solve the least squares problem with this basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the design matrix\n",
    "Phi = get_rbf_design_matrix(X, x_centers, ell)\n",
    "# Solve least squares problem\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "# Make prediction at a dense set of points\n",
    "xx = np.linspace(0, 60, 200)\n",
    "Phi_xx = get_rbf_design_matrix(xx[:, None], x_centers, ell)\n",
    "Y_p = np.dot(Phi_xx, w_LS)\n",
    "# Plot the predictions\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(xx, Y_p, label='LS Prediction (Radial Basis Functions)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Experiment with different values of ell and centers.\n",
    "+ When are we underfitting?\n",
    "+ Which one (if any) gives you the best fit?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
