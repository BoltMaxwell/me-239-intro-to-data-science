{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 11.3: Decision making\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To demonstrate how to use the results of a binary classifier to make decisions\n",
    "\n",
    "## High melting explosives sensitivity\n",
    "Let's repeat what we did for the HMX example, but after splitting the dataset into training and validation subsets.\n",
    "We will be making predictions on the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data file:\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/hmx_data.csv'\n",
    "download(url)\n",
    "# Load the data using pandas\n",
    "import pandas as pd\n",
    "data = pd.read_csv('hmx_data.csv')\n",
    "# Extract data for regression\n",
    "# Heights as a numpy array\n",
    "x = data['Height'].values\n",
    "# The labels must be 0 and 1\n",
    "# We will use a dictionary to indicate our labeling\n",
    "label_coding = {'E': 1, 'N': 0}\n",
    "y = np.array([label_coding[r] for r in data['Result']])\n",
    "data['y'] = y\n",
    "\n",
    "# Separate data into training and validation\n",
    "num_obs = x.shape[0]\n",
    "# Select what percentage you want to put in the training data\n",
    "train_percentage = 0.7\n",
    "# Figure out how many training points you are going to use:\n",
    "num_train = int(num_obs * train_percentage)\n",
    "# Figure out how many validation points you are going to use:\n",
    "num_valid = num_obs - num_train\n",
    "print('num_train = {0:d}, num_valid = {1:d}'.format(num_train, num_valid))\n",
    "\n",
    "# Before splitting the data, randomly permute rows\n",
    "permuted_data = np.random.permutation(data)\n",
    "# Split\n",
    "train_data = permuted_data[:num_train] # This picks the first n_train rows\n",
    "valid_data = permuted_data[num_train:] # This puts the rest on the validation rows\n",
    "# Get the x's and the y's for regression\n",
    "x_train = train_data[:, 0].astype(np.float)\n",
    "y_train = train_data[:, 2].astype(np.int)\n",
    "x_valid = valid_data[:, 0].astype(np.float)\n",
    "y_valid = valid_data[:, 2].astype(np.int)\n",
    "# Let's plot the training and the validation datasets in different colors\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_valid, y_valid, 'o', label='Validation data')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)\n",
    "\n",
    "# Make the design matrix\n",
    "degree = 2\n",
    "Phi_train = get_polynomial_design_matrix(x_train[:, None], degree)\n",
    "model = LogisticRegression(penalty='none', fit_intercept=False).fit(Phi_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make probabilistic predictions on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_valid = get_polynomial_design_matrix(x_valid[:, None], degree)\n",
    "predictions = model.predict_proba(Phi_valid)\n",
    "print('x\\tp(y=0|x)\\tp(y=1|x)\\tTrue label')\n",
    "print('-' * 80)\n",
    "for i in range(x_valid.shape[0]):\n",
    "    print('{0:1.2f}\\t{1:1.2f}\\t\\t{2:1.2f}\\t\\t{3:d}'.format(x_valid[i], predictions[i, 0],\n",
    "                                                       predictions[i, 1], y_valid[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a nice way to visualize these probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(x_valid.shape[0]):\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.bar(np.arange(2), predictions[i])\n",
    "    ax.set_title(r'$x={0:1.2f}$ cm, True result = {1:d}'.format(x_valid[i], y_valid[i]))\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(model.classes_)\n",
    "    ax.set_ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to pose and solve the decision-making problem.\n",
    "We just need to define a cost matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_00 = cost of correctly picking 0 when 0 is true\n",
    "# c_01 = cost of wrongly picking 0 when 1 is true\n",
    "# c_11 = cost of correctly picking 1 when 1 is true\n",
    "# c_10 = cost of wrongly picking 1 when 0 is true\n",
    "cost_matrix = np.array(\n",
    "[[0.0, 1.0],\n",
    " [1.0, 0.0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that computes the expected cost of each choice given the predicted probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_cost(cost_matrix, prediction_prob):\n",
    "    res = np.zeros((2,))\n",
    "    for i in range(2):\n",
    "        res[i] = cost_matrix[i, 0] * prediction_prob[0] + cost_matrix[i, 1] * prediction_prob[1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstration, here is the expected cost of each decision for the first few validation points.\n",
    "We will put a * next to the choice with minimum cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x\\tCost of 0\\tCost of 1\\tTrue label\\tChoice')\n",
    "print('-' * 80)\n",
    "for i in range(x_valid.shape[0]):\n",
    "    exp_c = expected_cost(cost_matrix, predictions[i])\n",
    "    line = '{0:1.2f}\\t{1:1.2f}'.format(x_valid[i], exp_c[0])\n",
    "    tmp = '\\t\\t{0:1.2f}'.format(exp_c[1])\n",
    "    correct_choice = True\n",
    "    if exp_c[0] < exp_c[1]:\n",
    "        line += '*'\n",
    "        if y_valid[i] == 1:\n",
    "            correct_choice = False\n",
    "    else:\n",
    "        tmp += '*'\n",
    "        if y_valid[i] == 0:\n",
    "            correct_choice = False\n",
    "    line += tmp + '\\t\\t{0:d}'.format(y_valid[i])\n",
    "    if correct_choice:\n",
    "        line += '\\t\\tCORRECT'\n",
    "    else:\n",
    "        line += '\\t\\tWRONG'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most of the choices are correct. But there are some wrong choices.\n",
    "The particularly bad wrong choices are the ones where we predict 0 (no explosion), but there is actually an explosion. Are there any such cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me now show you another very nice way to compute the expected cost for all the validation points in one line.\n",
    "This is using the [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) function (Einstein summation convention).\n",
    "It takes a while to understand what it does, but if you do you can shorten by a lot your linear algebra code.\n",
    "The idea is that repeated indices are summed over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cost = np.einsum('ij,kj->ki', cost_matrix, predictions)\n",
    "print(exp_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is yet another way to visualize the decisions of binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x_valid.shape[0]):\n",
    "    # Make decision\n",
    "    decision = model.classes_[np.argmin(exp_cost[i])]\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.bar(np.arange(2), predictions[i])\n",
    "    ax.set_title(r'$x={0:1.2f}$ cm, True result = {1:d}, Decision = {2:d}'.format(x_valid[i], y_valid[i], decision))\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(model.classes_)\n",
    "    ax.set_ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the decision boundary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of decision boundary\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "pE = np.linspace(0, 1, 100)\n",
    "pN = 1.0 - pE\n",
    "probs = np.hstack([pN[:, None], pE[:, None]])\n",
    "exp_cost = np.einsum('ij,kj->ki', cost_matrix, probs)\n",
    "decision_idx = np.argmin(exp_cost, axis=1)\n",
    "ax.plot(pE, decision_idx)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels(['N', 'E'])\n",
    "ax.set_ylabel('Decision')\n",
    "ax.set_xlabel('Predictive probability of E');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Repeat the analysis above with a different cost matrix that penalizes more calling a non-explosion when there is an explosion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
