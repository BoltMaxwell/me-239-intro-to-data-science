{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "# The following code is a modification of the code found here:\n",
    "# https://stackoverflow.com/questions/35651932/plotting-img-with-matplotlib\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.cbook import get_sample_data\n",
    "def imscatter(x, y, images, cmap=plt.cm.gray_r, ax=None, zoom=1):\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0, image in zip(x, y, images):\n",
    "        im = OffsetImage(image, zoom=zoom, cmap=cmap, interpolation='nearest')\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 13.2 - Clustering High-dimensional Data\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Combine principal component analysis with k-means to solve high-dimensional clustering problems\n",
    "\n",
    "In this hands-on activity we are going to cluster the MNIST dataset.\n",
    "We cannot apply K-means directly on it because of its high-dimensionality. If we did, we would get garbage.\n",
    "Instead, we are going to first reduce the dimensionality of MNIST using PCA to two dimensions and then we will apply K-means on the principal components.\n",
    "\n",
    "Note that, in contrast to the previous hands-on activity, we are going to work with the entire training set and not just one digit.\n",
    "So, we know that there are 10 clusters (the digits from 0 to 9).\n",
    "Let's see if the process we follow identifies clusters that correspond to digits...\n",
    "Here we go. First, download and load the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfortm PCA on the entire data set keep two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "vectorized_x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
    "pca = PCA(n_components=2, whiten=True).fit(vectorized_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Let's now visualize the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "Z = pca.transform(vectorized_x_train[:3000])\n",
    "imscatter(Z[:, 0], Z[:, 1], x_train[:3000], ax=ax, zoom=0.2)\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visually observe that PCA somewhat separates the digists.\n",
    "It's not perfect (and you can do better with non-linear dimensionality reduction techniques), but it would do for now.\n",
    "\n",
    "Now it's time for K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cl = KMeans(n_clusters=10).fit(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "for i, c in zip(range(10), ['Greys_r', 'Blues_r', 'BrBG', 'BuGn', 'BuPu', 'Greens_r', 'Dark2', 'GnBu',\n",
    "                           'Reds_r', 'Set1', 'Spectral']):\n",
    "    idx = cl.labels_[:3000] == i\n",
    "    imscatter(Z[:3000][idx, 0], Z[:3000][idx, 1], x_train[:3000][idx], cmap=c, ax=ax, zoom=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice! Observe that the the clusters look very reasonable.\n",
    "Again, they are not perfect but the make sense.\n",
    "Even in the cases that we know are not classified correctly, the errors are not ridiculously bad. As a matter of fact, the results are quite impressive if one takes into account that the algorithm we have put together does not know what digits are...\n",
    "\n",
    "Okay. Now let's look at the cluster centers a bit more closely.\n",
    "Let's visualize them as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the cluster means look like?\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots(dpi=28)\n",
    "    ax.imshow(pca.inverse_transform(cl.cluster_centers_[i:(i+1), :]).reshape((28,28)),\n",
    "                                    cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Which cluster centers are digits and which aren't digits? Pick one of the non-digists clusters and figure out where it is. You can do this by looking at ``cl.centers_`` to identify the coordinates of the center. Why was it picked? Does its shape make sense now?\n",
    "\n",
    "+ Repeat the analysis above but using 3 principal components (instead of 2). (Note that the 2D visualization of the principal components will not make much sense now, so take it with a grain of salt.) Pay special attention the identified cluster centers as images. Better or worse than before?\n",
    "\n",
    "+ Repeat the analysis with 5 principal components.\n",
    "\n",
    "+ Repeat the analysis with 200 principal components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
